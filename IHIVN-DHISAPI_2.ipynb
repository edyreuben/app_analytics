{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e28d72-803b-489e-b659-86b7add042b3",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1871a9-98db-4b6f-ba3f-bb80a2644d97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab57cc96-7134-4522-b957-75692b690cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import libraries for HTTP requests, authentication, and URL handling\n",
    "import requests                           # -- For making HTTP requests to the DHIS2 API\n",
    "from requests.auth import HTTPBasicAuth   # -- For handling basic authentication in HTTP requests\n",
    "from urllib.parse import quote            # -- For URL encoding special characters\n",
    "from getpass import getpass               # -- For securely collecting the passkey (hidden input)\n",
    "\n",
    "# -- Import data manipulation and numerical libraries\n",
    "import pandas as pd                       # -- For data manipulation and creating dfs\n",
    "import numpy as np                        # -- For data manipulation and numerical operations\n",
    "\n",
    "# -- Import Jupyter Notebook display and interactive utilities\n",
    "from IPython.display import clear_output  # -- For clearing screen display in Jupyter\n",
    "from IPython.display import display       # -- For displaying dfs in Jupyter Notebooks\n",
    "import ipywidgets as widgets              # -- For creating interactive buttons and widgets\n",
    "from functools import partial             # -- For cleaner argument binding in function calls\n",
    "\n",
    "# -- Import file, system, and regular expression utilities\n",
    "import os                                 # -- For operating system interactions (e.g., file paths)\n",
    "import sys                                # -- For system-specific parameters and functions\n",
    "import re                                 # -- For working with regular expressions\n",
    "\n",
    "# -- Import Excel file manipulation and styling tools\n",
    "from openpyxl import load_workbook        # -- For working with Excel files\n",
    "from openpyxl.styles import Font, Alignment  # -- For styling and aligning Excel cells\n",
    "\n",
    "# -- Import document and image processing libraries\n",
    "from docx import Document                # -- For creating and editing Word documents\n",
    "from docx.shared import Pt, RGBColor, Inches  # -- For Word document styling (e.g., font size, color, dimensions)\n",
    "from html2image import Html2Image        # -- For converting HTML to images\n",
    "from PIL import Image                    # -- For image processing\n",
    "import pdfkit                            # -- For generating PDFs from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa0c0f-00b3-4dbe-b484-004c5b19d80c",
   "metadata": {},
   "source": [
    "#### - IHVN DHIS2 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cdea3c-7a09-416f-ab5a-60c1b5d0b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to fetch and process DHIS2 data\n",
    "def fetch_and_process_DHIS2_data(username, password, start_period, end_period, named_urls=None):\n",
    "    \"\"\"\n",
    "    Fetch and process DHIS2 data from named URLs with a user-specified period range, returning a dictionary of processed dfs.\n",
    "    \n",
    "    Args:\n",
    "        username (str): DHIS2 username for authentication\n",
    "        password (str): DHIS2 password for authentication\n",
    "        start_period (str): Start period in YYYYMM format (e.g., '202501')\n",
    "        end_period (str): End period in YYYYMM format (e.g., '202503')\n",
    "        named_urls (dict, optional): Dictionary where keys are names (e.g., 'Report_Rate_Facility') and values are DHIS2 API URLs.\n",
    "                                    If None, a default set of URLs is used.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary where keys are URL names and values are processed dfs\n",
    "    \"\"\"\n",
    "    # -- Step 4: Define separator line\n",
    "    separator_line = '-' * 43                                 # -- Create a separator line of 43 dashes\n",
    "    \n",
    "    # -- Step 1: Define default DHIS2 URLs if none are provided\n",
    "    if named_urls is None:                                    # -- Check if named_urls is not provided\n",
    "        named_urls = {                                        # -- Define default DHIS2 URLs\n",
    "            \"Report_Rate_Facility\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3AKH62ia35VIZ%3Bum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AZ7E9RxXmwxG.REPORTING_RATE%3BVmGwLcfPS2N.REPORTING_RATE%3BYFnIy7lATQL.REPORTING_RATE%3BNkuV7xoThHV.REPORTING_RATE%3BHwfLR3npibF.REPORTING_RATE%3BvN9rk5ChByM.REPORTING_RATE%3BoxUN7AXSF8r.REPORTING_RATE&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false\",\n",
    "            \"Report_Rate_LGA\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-lmSTo2yxNsA&dimension=dx%3AZ7E9RxXmwxG.REPORTING_RATE%3BVmGwLcfPS2N.REPORTING_RATE%3BYFnIy7lATQL.REPORTING_RATE%3BNkuV7xoThHV.REPORTING_RATE%3BHwfLR3npibF.REPORTING_RATE%3BvN9rk5ChByM.REPORTING_RATE%3BoxUN7AXSF8r.REPORTING_RATE&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"AGYW_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AQ1KKjeS4seJ%3BzhvcKIWKvEX%3BpgsZWQLbTvw%3BdPefeXOI0MT%3Bb62rwfvBP13%3Bechn4uCHBhF%3BeoSKe92wBYa%3BIYJciZgP6Yt%3BbbSqZH1OAxk%3By5mYZFQbMe6%3BSSa2P2O1keL%3BrtjZkt3ImND%3BDhw4lcmA8i5%3BAMs19im8mm7%3BTNTPcRPC3jV%3BWWaSbjutZof%3BcspEoTIBnOB%3BQ1ntMoY7ZrP%3BjQ51vKvy1SN%3BKj0TobAENyg%3BKRf4sYxv9KG&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"ART_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AEVJnWv5UQ2I%3BaDFx7U0OSNp%3BuTAVBA24Qgg%3BVNvZaoEcS8M%3BdJpKA2CL66w%3BwUlUHsYzh80%3BmaTBR3htwav%3BN4skK4jJVnm%3BcMpe2pMLJed%3BmJ3Af4Qg0wV%3BxFsrvhyu0Wx%3BCVIR5mDSrr0%3BqO3FSAwqg15%3BE8J56tfMIEa%3BBiqkhMdFIwy%3BfVpRSB8jy9Q%3BudXxeZhT8Fd%3BE20mRpvl5jK%3Bgcg9I4dagWN%3BfBJzc5QIP1b%3BgDuNCzc5liq%3BngCJ4UZOCme%3BEtg6BPVX548%3BiTJ2VvKOWHG%3BrJB5XXrF5zx%3BFRMmrIYSRfz%3BPhUxFwPj2US%3BKtMzH6OTxXL%3Bu0W1SpovSd3%3Bxz9C4uZwMuB%3BLqdahCtUzSX%3Bqb3YzC5X9Lo%3BbFW6JaVxyOo%3BTSzdU77XfK2&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"HTS_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AsxXB2RokZrt%3BhxYcq0LyXh6%3Bi3I5wU8vm0U%3BwSLnU5ihgb3%3BVz0ZjXCnFtX%3BK7Bkwdae7X1%3BRBxwgKGZYpv%3BjNodLWC4U4d%3Bq6bey1tg06I%3BAZSZngrMzj6%3BpS0Cjik4WEH%3BBrVqJd9MHSA%3BXKu24OQK64R%3BpgiqhOhD95N%3BYL87jqnIzQA%3BmN5TqnUQRgq%3BEvB0bzvxMq0%3BgWVJbVYOT1A%3Bd8lMQuQpLPe%3BT6f8BA7M9Q0%3BSVpx1fVD3bh%3BpJH2bCApdTe%3BuGUbRjQvCeN%3BE3bZgIIG5qQ%3Bd8pnVkpyGlU%3BDn4PJOJJASf%3BQV3wq0WsLXe%3Bs9ksVidbnRG%3BOFj0H56KbKp%3Bh9SMVFhNbBN%3BrJD9ER08iT4%3BJLOnLNuLOva%3BZztEVzPBwwl%3BX6wyczqYmJN%3Bvhhrj8rTq83%3ByR5JEOs2t7q%3BCnSrjlY5Siw%3BUMPLBlGYjWM%3BiE5kXuUJoMC%3BuMqKjodBP76%3Bu9gp0VFbnHh%3BzJuaAQbUlq8%3BQXNyy0dbIpd%3BomaL0XteWO8%3BKtzvesDYABJ%3BHdTQzlvMtjU%3Bqznyc2H90Ay%3Bdtri8UEwZFV%3BFB1EZfVSmYi%3BXzghRlcLMqB%3BOk6fJzfTk75%3BomujB6405jI%3BenPRkAVrHKi%3BE6wddhqJVIN%3BjWajcYCyiDE%3BLSQuUgoTH7o%3Bilp3JWW1qt5%3BURgQ4d3y8WF%3Bxc40kwzTBVG%3BmCD2Qhbd6CY%3BnQtTVOdEMqV%3BKQAEkHXQe7N%3BxBr9Sgyk4cO%3BlI7YzdC7wEd%3BegRRIwWxnJs%3BPPxR0HhCKQR%3BkmBNFDE8duJ%3BGTUzO3HGLWA%3Bh8HFl5EeHHl%3BuXwAyGT9eqW%3BDahOUj6bRk0%3BT8G2KNNZ4eI%3BD7ygy6yCHFs%3BuSQJcqAEvHg%3BU74jSwLCoA1%3BZheiLjTrqRZ%3BjlFslOk3bkU%3BOxBKckVqp29%3BJWHc7D4J132%3Bq1XlBEcB1PE%3BNb95zNUKf29%3BevlfYhoKrjI%3BU8xSE6OneYl%3BF2JEmiJt3Yl%3BPZWrrb7VyCj%3BXxpFcHK1S89%3BsICfdv3or5G%3Bg5HvgkCKSSU%3Bh7XmbTNyTUi%3BUi7DiLwSgqm%3BHIHQHXPOGKl%3Bl3aXhanFimZ%3BSl6d3hqzq2C%3BpIsmPa1GjFs%3BOwrvPMKq0pQ%3BV7hcDYvuPMY%3BDWb8URoPRym%3BL7l30ySaQDy%3BnYrdLtwDlV0%3BifE9LKaLqUm%3BuMkZMIHVVjV%3BcCLFgRUkIww%3BHUWYU2ruloN%3BoXsckqTpzhN%3Bm4CzDuc50Jn%3BLPyxdv2eBEj%3BZLARI5LYBOL%3BPYL5GdQGPfI%3BmQZ4z94jzak%3BRkwT0w0mPNj%3BEC5iN37lZ61%3BS0QI1IcESjq%3BnR3ZklnEuBy%3BgxZKoTwyLnn%3BIp75Jb7o8Au%3BVPMZR303TWP%3BjqP0tN3v5hJ%3BuoBeVy413Mj%3BPn1GooQL0I4&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"HTS_MSF_HIVST_approach\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AT8G2KNNZ4eI&dimension=tBdRxXi3Dxr%3AALL_ITEMS&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"NSP_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3AxesSKTdzhPF%3BuIx4sChOeMC%3BkJSLUKrBKez%3BJylie7CPg63%3BTV5DhhOgF7s%3BL0X8cbiEfrC%3BoSDqDrfRpb3%3BIQYpNyC6lF2%3BlOVVI8B4Ag6%3BRxQv56EIuWs%3BqyV24hEIgM5%3BObJbRGQ3QPI%3BptgY5CK3GUc%3BC16TIH6Zxju%3BKY2gb90cvTu%3BO3g2Lq9fpUU%3Bkk3hEztWa48%3BxvfqoSSOIOL%3BcLSlAzBug6Y%3BayTk1t8sjFN%3BjY2SLdSlMug%3Bv3sfwf9O1R9%3BxrHzg7SORIt&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"PMTCT_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202502&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3ABWLsXE490Yw%3BAtzuY8wFIaP%3BmjoPYWLowE6%3BhWkEco9hG0R%3BgybVHn9Y3SQ%3BkNEHSaIxOja%3BSWsoIgeQBKd%3Bym1WjkXu8ol%3BHDVcSJrN68V%3BIY3zDXL1t0b%3BCrv4RYmAJot%3Bn1MZAkrNWlM%3BGDFB54Uuepj%3BPaXizDbvXcE%3BhnqKzrB2hX0%3BZv0Go6hoDxZ%3BZOzgBnzWLOK%3Bbj8jc0HujI2%3BVlhKdJD8Mav%3BqO3KHhkVM38%3BkSHVwcO9iYZ%3BWnEhg2SiMjF%3BFR9ibOgyNKg%3BbyiZy4Xo7Rx%3BvpEeS23uOzB%3BzoSht7Xk3OI%3BUMFtzIDX9nE%3BxkQqd3C5vd8%3BCAO8uuth3q6%3BRATzsmd7JfL%3BDPFGLfkzZTF%3BZZEXn1RzLwL%3BuMATgKm1ZEV%3ByShXhGc00TY%3Bdc0p7yc1jPy%3BJsyAmKQMqK1%3BlIJwQ20CgPE%3BbxeHOqH0s2V%3BOum5ofYWexC%3BadqiOXDSRbU%3BHFq8NihMhgw%3BcibdW4TLOJ5%3BMJHhWCCOy1q%3BPQHBZmMVWE3%3Bw9NSJbXp5Ub%3BCyrYJsUOe7Q%3BER9cFdvluIM%3BUP3mRp6Zr69%3Bgl5kyYu85zX%3BrIY8zpZJxtv%3BaPtwtDwhOeL%3BuQEZL5j8ZHS%3BSpuoRhODIuZ%3Bvu5TP1CuesM%3BeZ4kUaPlQWC%3BSVDAcLT6IKS%3BFLt9JLhBWlK%3Bt9NJ4JhsXVl%3BnOpah0JpIzI%3BDuFJekp1ymh%3BTJ5BREHjZnu%3BWzAmqotjxV7%3BwRRoicp2S4Y%3Bzq6Tvgk5GFl%3BLMrI0jXhjP4%3BTDLVNVgHUvf%3BakVDx9ew0y8%3ByDAbvAlPR69%3Bi6rCuewXarq%3BzKYuhIKhMKM%3BGQ4kUNGMf7i%3Bv2e5LaziiBr%3BsLWmlnDQzgt%3BrKzvhBgkpe5%3BpyUW87YFbY5%3BssvxYtnGXpp%3BlY6wzD8718l%3BeLZuFwiv4rQ%3BDRFLRwv7f8F%3BLgbHzzQILZY%3BWt8jhQxqq9L%3BTdqRbqoS3Uj%3BcfUpkarx8og%3BWbRBSkKaTE8%3BEvZz6jNSRMX%3BYGZr2K5Y0Yq%3BJGj4bfhtzCk%3BkKSgWbeTAL1%3Bo9zPtZI4c9T%3BcmkwuM6ZOaN%3BcGPBuUrZ5Oi%3BN4WysYNBRFk%3Bd0izXgJhn2P%3BxUYwPZH7ulJ%3BykZcKOVoJeH%3Bj3ycCFq6vzR%3BK2lEmtE4xjz%3Bw4jlyeHkVTD%3BDbj5r3zevDF&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\",\n",
    "            \"KP_Prev_MSF\": \"https://ihvn.dhistance.com/api/analytics.json?dimension=pe%3A202501&dimension=ou%3Aum5TFmcsSi8%3BLEVEL-UIlRiekzsf6&dimension=dx%3ArJFps9PGgYI%3BpFrm4E6yE0i%3Bwld8ChYjUUS%3Bhnqs8zD2RBz%3BFdogfvZ9Es5%3BUWnjd2hDMpZ%3BmM3TZYEkMaF%3BK93mvkTXpbl%3BtIZvjax5TD5%3BN05zqacXFsD%3BcVHdvMKuwse%3BBFSM6D4KWg4%3ByHBgGkS0dPr%3BEw3ZRxdYY7g%3BihRnS748lOC%3BlIu2vmR2zHU%3BfbxvYGgVsrK%3BTdATue0b5th%3BFBBKTzFqBVR%3BF27CfSgCmSk%3BdecdNlpZy4B%3BRVuw3Ny3ysw%3BhN5ZT3zziqA%3BKngVqiOqAdp%3Bdfznt6rhrYg%3BbxK7Uh32HMK%3BduzVXts1QUh%3BXpDOj1a7ZdC%3BogXBJLCw1Qe%3BAKQTKPoUHt6%3BrClFr1FdYVY%3BdTWbSB6HZg6%3BGZo5a5CwzV7%3BdIElcrfEgSB%3BP8oCCg2mUcG%3BZc7dmnNEQwj%3BmWQsKiSyv7J%3BQVOL72RTOZR%3Bri9iXOK3wxX%3Bpqp8Tyvsdc1%3BNat3LtmEWXD%3BPm78aQ5UVh8%3BWaI8nE16Ab0%3BfHlNG1CMqYX%3BGwxEq7jrcSc%3BiqqfsLcdWiK%3BeF7Yi0cLTXP%3BVnDrAREzUE2%3BSSxprjJB8vd%3BL5pAPbPCrNF%3BPrdKtyHsOE8%3BLNQtCiSCfnC%3Bb3JIlAZUjNJ%3BeK3YlBsHGkB%3BTz9WrAjxVY9%3BMkl91koS5cn%3BdAQlguHfLF8%3BBdgtqOw1P33%3BhZzpfxOQo4b%3BJBoik13RWdP%3BhAVQ3sTq3nJ%3BGKiJWrIeEJu%3BqqFvQly7I9h%3BJtOlLSk4yTF%3BUfMiT3LdXYc%3BdLj7PgakBi5%3BUaOdcjrcI59%3BssQZNWweFrL%3BhHkzZLsAqmI%3BkYAhnVk8fYp%3BkTIGrSkqbrG%3Bl4ZD3aaxH7j%3BWAZm2HXm3xI%3BSECmJH5Lfer%3BKHnHX7Rb29d%3BVE0Z777wXB2%3Bzgqte2mbKxT%3BDbavrmvoWPG%3BJgDiY0dv2RX%3BS8WFXWTFvai%3BBBwzjs2JQ1Z%3BxRJrWUdpAxm%3BYS8QAyBakNp%3BJfPSsLKaeDV%3BkmsvoBtZ4oa&showHierarchy=true&hierarchyMeta=true&includeMetadataDetails=true&includeNumDen=true&skipRounding=false&completedOnly=false&outputIdScheme=UID\"\n",
    "        }\n",
    "\n",
    "    # -- Step 2: Extract dx IDs from named_urls\n",
    "    all_dx_ids = set()                                        # -- Initialize empty set to store unique dx IDs\n",
    "    for url in named_urls.values():                           # -- Iterate over each URL in named_urls\n",
    "        dx_matches = re.findall(r'dx%3A([^&]+)', url)         # -- Extract dx parameter from URL using regex\n",
    "        for match in dx_matches:                              # -- Process each dx match found\n",
    "            ids = [id.split('.')[0] for id in match.split('%3B')]  # -- Split by %3B and remove .REPORTING_RATE suffixes\n",
    "            all_dx_ids.update(ids)                            # -- Add cleaned IDs to the set\n",
    "\n",
    "    # -- Step 3: Fetch only relevant data element descriptions\n",
    "    base_url = 'https://ihvn.dhistance.com/api/dataElements'  # -- Set base URL for data elements endpoint\n",
    "    dx_filter = f\"id:in:[{','.join(all_dx_ids)}]\"             # -- Create filter string for specific dx IDs\n",
    "    params = {                                                # -- Define query parameters for the API request\n",
    "        'fields': 'id,name,description',                      # -- Request id, name, and description fields\n",
    "        'filter': dx_filter,                                  # -- Apply filter for specific IDs\n",
    "        'paging': 'false'                                     # -- Disable pagination to get all results in one response\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, auth=HTTPBasicAuth(username, password), params=params)\n",
    "        response.raise_for_status()\n",
    "        data_elements = response.json().get('dataElements', [])\n",
    "        print(f\"Fetched {len(data_elements)} data elements\")\n",
    "        dataelement_to_description = {de['id']: de.get('description', de['name']) for de in data_elements}\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 401:\n",
    "            print(separator_line)\n",
    "            print(f\"⦸ Error: Failed to fetch descriptions: IHVN DHIS2 login credentials invalid\")\n",
    "            print(separator_line)\n",
    "            return\n",
    "        print(f\"⦸ Error: Failed to fetch descriptions: IHVN DHIS2 login credentials invalid\")\n",
    "        dataelement_to_description = {}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⦸ Error: Failed to fetch descriptions: IHVN DHIS2 login credentials invalid\")\n",
    "        dataelement_to_description = {}\n",
    "\n",
    "    # -- Step 5: Validate and parse the start and end periods\n",
    "    # -- Step 5.1: Validate start and end periods\n",
    "    for period in [start_period, end_period]:                 # -- Check both start and end periods\n",
    "        if not (isinstance(period, str) and len(period) == 6 and period.isdigit() and 1 <= int(period[4:]) <= 12):  # -- Validate format and month range\n",
    "            print(separator_line)                             # -- Print separator line for readability\n",
    "            print(\"⦸ Error: Invalid period date format\")      # -- Print error message\n",
    "            print(separator_line)                             # -- Print separator line for readability\n",
    "            return {}                                         # -- Return empty dict to halt execution\n",
    "\n",
    "    # -- Step 5.2: Parse the start and end periods\n",
    "    start_year = int(start_period[:4])                        # -- Extract year from start period (e.g., 2025)\n",
    "    start_month = int(start_period[4:])                       # -- Extract month from start period (e.g., 01)\n",
    "    end_year = int(end_period[:4])                            # -- Extract year from end period (e.g., 2025)\n",
    "    end_month = int(end_period[4:])                           # -- Extract month from end period (e.g., 03)\n",
    "\n",
    "    # -- Step 6: Format the period range for display\n",
    "    month_names = {                                           # -- Define month names for display\n",
    "        1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "        7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "    }\n",
    "    start_display = f\"{month_names[start_month]}{str(start_year)[-2:]}\"  # -- Format start period (e.g., Jan-25)\n",
    "    end_display = f\"{month_names[end_month]}{str(end_year)[-2:]}\"        # -- Format end period (e.g., Mar-25)\n",
    "\n",
    "    # -- Step 7: Print separator line to highlight successful setup\n",
    "    print('Data processed and stored as:')                    # -- Print processing start message\n",
    "    print(separator_line)                                     # -- Print separator line after message\n",
    "\n",
    "    # -- Step 8: Generate a list of periods between start and end\n",
    "    periods = []                                              # -- Initialize empty list for periods\n",
    "    current_year, current_month = start_year, start_month     # -- Set starting point for period generation\n",
    "    while (current_year < end_year) or (current_year == end_year and current_month <= end_month):  # -- Loop until end period is reached\n",
    "        periods.append(f\"{current_year}{current_month:02d}\")  # -- Add period in YYYYMM format (e.g., 202501)\n",
    "        current_month += 1                                    # -- Increment month\n",
    "        if current_month > 12:                                # -- If month exceeds 12\n",
    "            current_month = 1                                 # -- Reset to January\n",
    "            current_year += 1                                 # -- Increment year\n",
    "\n",
    "    # -- Step 9: Encode the periods for URL use\n",
    "    period_string = \"%3B\".join(periods)                       # -- Join periods with %3B (URL-encoded semicolon)\n",
    "    period_param = f\"dimension=pe%3A{period_string}\"          # -- Format period parameter for API URL\n",
    "\n",
    "    # -- Step 10: Initialize data storage and counters\n",
    "    processed_data = {}                                       # -- Initialize dict to store processed DataFrames\n",
    "    success_count = 0                                         # -- Counter for successful URL processes\n",
    "    total_urls = len(named_urls)                              # -- Total number of URLs to process\n",
    "\n",
    "    # -- Step 11: Define cluster mapping for LGAs\n",
    "    cluster = {                                               # -- Define mapping of LGAs to clusters\n",
    "        \"an Aguata\": \"Aguata\", \"an Anaocha\": \"Aguata\", \"an Orumba North\": \"Aguata\", \"an Orumba South\": \"Aguata\",\n",
    "        \"an Awka North\": \"Awka\", \"an Awka South\": \"Awka\", \"an Dunukofia\": \"Awka\", \"an Idemili North\": \"Awka\",\n",
    "        \"an Idemili South\": \"Awka\", \"an Njikoka\": \"Awka\", \"an Ekwusigo\": \"Nnewi\", \"an Ihiala\": \"Nnewi\",\n",
    "        \"an Nnewi North\": \"Nnewi\", \"an Nnewi South\": \"Nnewi\", \"an Anambra East\": \"Omambala\",\n",
    "        \"an Anambra West\": \"Omambala\", \"an Ayamelum\": \"Omambala\", \"an Oyi\": \"Omambala\", \"an Ogbaru\": \"Onitsha\",\n",
    "        \"an Onitsha North\": \"Onitsha\", \"an Onitsha South\": \"Onitsha\"\n",
    "    }\n",
    "\n",
    "    # -- Step 12: Process each named URL\n",
    "    for url_name, url in named_urls.items():                  # -- Iterate over each name-URL pair\n",
    "        # -- Step 12.1: Update URL with new period range\n",
    "        if \"dimension=pe%3A\" in url:                          # -- Check if URL has a period dimension\n",
    "            start_idx = url.find(\"dimension=pe%3A\")           # -- Find start of period parameter\n",
    "            end_idx = url.find(\"&\", start_idx) if url.find(\"&\", start_idx) != -1 else len(url)  # -- Find end of period parameter\n",
    "            url = url[:start_idx] + period_param + url[end_idx:]  # -- Replace old period with new one\n",
    "        else:                                                 # -- If no period dimension exists\n",
    "            url = url + \"&\" + period_param if \"?\" in url else url + \"?\" + period_param  # -- Append period parameter\n",
    "\n",
    "        # -- Step 12.2: Fetch data from DHIS2 API\n",
    "        try:\n",
    "            response = requests.get(url, auth=HTTPBasicAuth(username, password))  # -- Send GET request with authentication\n",
    "            response.raise_for_status()                       # -- Raise exception if request fails\n",
    "            data = response.json()                            # -- Parse response into JSON\n",
    "        except requests.exceptions.RequestException as e:     # -- Catch request-related exceptions\n",
    "            print(f\"⦸ Error: No signal to get '{url_name}' data\")  # -- Print error message\n",
    "            continue                                          # -- Skip to next URL\n",
    "\n",
    "        # -- Step 12.3: Extract table structure from JSON\n",
    "        headers = [header['name'] for header in data.get('headers', [])]  # -- Get column names from headers\n",
    "        df = pd.DataFrame(data.get('rows', []), columns=headers)  # -- Create DataFrame from rows\n",
    "        df = df.rename(columns={'dx': 'dataElement', 'ou': 'orgUnit', 'pe': 'period'})  # -- Standardize column names\n",
    "\n",
    "        # -- Step 12.4: Extract metadata from the JSON response\n",
    "        meta = data.get('metaData', {})                       # -- Get metadata section\n",
    "        ou_hierarchy = meta.get('ouHierarchy', {})            # -- Get organizational unit hierarchy\n",
    "        items = meta.get('items', {})                         # -- Get item mappings (IDs to names)\n",
    "\n",
    "        # -- Step 12.5: Create organizational unit mappings based on URL name\n",
    "        if url_name == 'Report_Rate_LGA':                     # -- Special case for Report_Rate_LGA\n",
    "            orgunit_to_level = {ou: ou for ou in df['orgUnit'].unique()}  # -- Map orgUnit to itself\n",
    "        else:                                                 # -- For other URLs\n",
    "            orgunit_to_level = {                              # -- Extract second level from hierarchy\n",
    "                ou: ou_hierarchy.get(ou, '').split('/')[1] if '/' in ou_hierarchy.get(ou, '') else ou\n",
    "                for ou in df['orgUnit'].unique()              # -- Iterate over unique orgUnits\n",
    "            }\n",
    "\n",
    "        # -- Step 12.6:  Create name mappings for organizational units and data elements\n",
    "        level_to_name = {                                     # -- Map orgUnit level IDs to names\n",
    "            org_id: items[org_id]['name']\n",
    "            for org_id in set(orgunit_to_level.values()) if org_id in items  # -- Only include IDs in items\n",
    "        }\n",
    "        orgunit_to_name = {                                   # -- Map orgUnit IDs to names\n",
    "            ou: items[ou]['name']\n",
    "            for ou in df['orgUnit'].unique() if ou in items   # -- Only include orgUnits in items\n",
    "        }\n",
    "        dataelement_to_name = {                               # -- Map dataElement IDs to their names\n",
    "            de: items[de]['name']\n",
    "            for de in df['dataElement'].unique() \n",
    "            if de in items                                    # -- Only include dataElements present in items\n",
    "        }\n",
    "\n",
    "        # -- Step 12.7: Pivot the df to reshape the data\n",
    "        if url_name == 'HTS_MSF_HIVST_approach':              # -- Special case for HTS_MSF_HIVST_approach\n",
    "            pivoted_df = df.pivot(                            # -- Pivot using tBdRxXi3Dxr column\n",
    "                index=['period', 'orgUnit'],                  # -- Set index columns\n",
    "                columns=['tBdRxXi3Dxr'],                      # -- Set pivot column\n",
    "                values='value'                                # -- Set value column\n",
    "            ).reset_index()                                   # -- Reset index to columns\n",
    "        else:                                                 # -- For other URLs\n",
    "            pivoted_df = df.pivot(                            # -- Pivot using dataElement column\n",
    "                index=['period', 'orgUnit'],                  # -- Set index columns\n",
    "                columns='dataElement',                        # -- Set pivot column\n",
    "                values='value'                                # -- Set value column\n",
    "            ).reset_index()                                   # -- Reset index to columns\n",
    "        pivoted_df.columns.name = None                        # -- Clear column index name\n",
    "\n",
    "        # -- Step 12.8: Format the 'period' column to 'Mon-YY' (e.g., Jan-24)\n",
    "        pivoted_df['period'] = pd.to_datetime(pivoted_df['period'], format='%Y%m').dt.strftime('%b-%y')  # -- Convert YYYYMM to Mon-YY\n",
    "\n",
    "        # -- Step 12.9: Add organizational hierarchy information to the pivoted df\n",
    "        pivoted_df['orgunitlevel'] = pivoted_df['orgUnit'].map(orgunit_to_level)  # -- Add orgUnit level\n",
    "        pivoted_df['LGA'] = pivoted_df['orgunitlevel'].map(level_to_name)         # -- Add LGA name\n",
    "        pivoted_df['orgUnit'] = pivoted_df['orgUnit'].map(orgunit_to_name)        # -- Replace orgUnit ID with name\n",
    "        pivoted_df['Cluster'] = pivoted_df['LGA'].map(cluster)                    # -- Add cluster mapping\n",
    "\n",
    "        # -- Step 12.10: Rename columns with dataelemenet descriptions\n",
    "        if url_name in ['Report_Rate_Facility', 'Report_Rate_LGA']:\n",
    "            rename_dict = {                                   # -- Create a dictionary for renaming columns\n",
    "                **{col: dataelement_to_name[col] for col in pivoted_df.columns if col in dataelement_to_name},  # -- Rename dataElement columns to their names\n",
    "                'period': 'ReportPeriod',                         # -- Rename 'period' to 'ReportPeriod'\n",
    "                'orgUnit': 'FacilityName'                         # -- Rename 'orgUnit' to 'FacilityName'\n",
    "            }\n",
    "        else:\n",
    "            rename_dict = {                                       # -- Create renaming dictionary\n",
    "                **{col: dataelement_to_description.get(col, col) \n",
    "                   for col in pivoted_df.columns if col in dataelement_to_description},  # -- Use descriptions for data elements\n",
    "                'period': 'ReportPeriod',                         # -- Rename period to ReportPeriod\n",
    "                'orgUnit': 'FacilityName'                         # -- Rename orgUnit to FacilityName\n",
    "            }\n",
    "        pivoted_df.rename(columns=rename_dict, inplace=True)      # -- Apply renaming\n",
    "\n",
    "        # -- Step 12.11: Handle NaN values\n",
    "        if url_name in ['Report_Rate_Facility', 'Report_Rate_LGA']:  # -- Check if URL is a report rate type\n",
    "            pivoted_df.fillna('', inplace=True)                   # -- Replace NaN with empty string\n",
    "        else:                                                     # -- For other URLs\n",
    "            pivoted_df.fillna(0, inplace=True)                    # -- Replace NaN with 0\n",
    "\n",
    "        # -- Step 12.12: Finalize DataFrame\n",
    "        pivoted_df = pivoted_df.reset_index(drop=True)            # -- Reset index and drop it\n",
    "        processed_data[url_name] = pivoted_df                     # -- Store processed DataFrame\n",
    "        success_count += 1                                        # -- Increment success counter\n",
    "        print(f\"- {url_name}\")                                    # -- Print URL name as processed\n",
    "\n",
    "    # -- Step 13: Display processing summary\n",
    "    global report_period_display\n",
    "    report_period_display = (                                 # -- Format summary message\n",
    "        f\"✔️ Data fetched successfully!\\nReport extracts: ({success_count}/{total_urls})\\n\"\n",
    "        f\"Extraction period: {start_display} - {end_display}\\nWorkbook variables & functions loaded\"\n",
    "    )\n",
    "    if processed_data:                                        # -- Check if any data was processed\n",
    "        print(separator_line)                                 # -- Print separator line before message\n",
    "        print(report_period_display)                          # -- Print success message\n",
    "        print(separator_line)                                 # -- Print separator line after message\n",
    "    else:                                                     # -- If no data was processed\n",
    "        print(separator_line)                                 # -- Print separator line before message\n",
    "        print(f\"⦸ Failed:\\nReport extracts: (0/{total_urls})\\nIHVN DHIS2 login credentials invalid\")  # -- Print failure message\n",
    "        print(separator_line)                                 # -- Print separator line after message\n",
    "\n",
    "    # -- Step 14: Return processed data\n",
    "    return processed_data                                     # -- Return dictionary of processed DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fc325-811f-4993-adb7-be36a0a6f304",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - Functions: Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fec3f2-1ea8-4bc6-824f-4eac8c914cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables():\n",
    "    \"\"\"\n",
    "    Defines variables for the notebook and assigns them as global.\n",
    "    \"\"\"\n",
    "    # Local imports to ensure dependencies are met\n",
    "    import os\n",
    "    \n",
    "    # Declare all variables as global\n",
    "    global file_path, report_name, report_name_rate, report_name_outlier, report_name_period\n",
    "    global report_name_period_name, report_period_name_folder, sub_folder_image_file, sub_folder_doc_file\n",
    "    global sub_folder2_image_file_report_rate, sub_folder2_image_file_msf_outlier\n",
    "    global doc_file_report_rate_xlsx, doc_file_msf_outlier_docx, doc_file_msf_outlier_xlsx\n",
    "    global highlight_red_list, MSF_hierarchy, MSF_report_rate_columns    \n",
    "\n",
    "    try:\n",
    "        # -- Step 1: Define main report export path\n",
    "        try:\n",
    "            file_path = r'C:\\Users\\HP\\Desktop\\ANSO Nnewi\\CQI\\python\\report\\msf\\ihvn'\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining file_path: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 2: Create report name, dynamic period, and joined report period name\n",
    "        try:\n",
    "            report_name = \"ANSO MSF report\"\n",
    "            report_name_rate = \"ANSO MSF report rate\"\n",
    "            report_name_outlier = \"ANSO MSF outlier\"\n",
    "            report_name_period = DHIS2_data['Report_Rate_Facility'].ReportPeriod.iloc[0]\n",
    "            report_name_period_name = f\"{report_name_period} {report_name}\"\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining report names or accessing DHIS2_data: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 3: Create report period folder\n",
    "        try:\n",
    "            report_period_name_folder = os.path.join(file_path, f\"{report_name_period_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error creating report_period_name_folder: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 4: Define folders for storing reports\n",
    "        try:\n",
    "            sub_folder_image_file = os.path.join(report_period_name_folder, \"image file\")\n",
    "            sub_folder_doc_file = os.path.join(report_period_name_folder, \"document file\")\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining sub_folder_image_file or sub_folder_doc_file: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 4.1: Add subfolders for specific report types\n",
    "        try:\n",
    "            sub_folder2_image_file_report_rate = os.path.join(sub_folder_image_file, f\"{report_name_rate}\")\n",
    "            sub_folder2_image_file_msf_outlier = os.path.join(sub_folder_image_file, f\"{report_name_outlier}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining sub_folder2_image_file_report_rate or sub_folder2_image_file_msf_outlier: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 5: Create folders if they do not exist\n",
    "        try:\n",
    "            os.makedirs(sub_folder_image_file, exist_ok=True)\n",
    "            os.makedirs(sub_folder_doc_file, exist_ok=True)\n",
    "            os.makedirs(sub_folder2_image_file_report_rate, exist_ok=True)\n",
    "            os.makedirs(sub_folder2_image_file_msf_outlier, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error creating directories: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 6: Define report document file paths\n",
    "        try:\n",
    "            doc_file_report_rate_xlsx = os.path.join(sub_folder_doc_file, f\"{report_name_period} {report_name_rate}.xlsx\")\n",
    "            doc_file_msf_outlier_docx = os.path.join(sub_folder_doc_file, f\"{report_name_period_name}.docx\")\n",
    "            doc_file_msf_outlier_xlsx = os.path.join(sub_folder_doc_file, f\"{report_name_period_name}.xlsx\")\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining document file paths: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 7: Define list of words and phrases to keep after HTML cleaning\n",
    "        try:\n",
    "            highlight_red_list = [\n",
    "                'Community', 'Walk-In', 'Community & Walk-In', 'subset of 4',\n",
    "                'Self, Spouse, Sexual Partner, Children, Social Network, Others',\n",
    "                'FSW, MSM, PWID, TG, Others', 'Testing frequency', 'Outreach',\n",
    "                'Outreach-Pregnant', 'Outreach-Others', 'Excluding community testing',\n",
    "                'Excluding previously known', 'IPV', 'ANC', 'L&D', '<72hrs PP',\n",
    "                '<72 hrs', '>72 hrs - < 6 months', '>6 - 12 months',\n",
    "                'ANC, L&D, <72hrs Post Partum', 'Facility', 'Outside Facility',\n",
    "                'Within and outside the facility', 'within 72 hrs of birth',\n",
    "                'between >72 hrs - <2 months of birth', 'All regimens', 'Regimen Lines',\n",
    "                'MMD', 'DSD', 'excludes ART transfer-in', 'ART Addendum-2'\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining highlight_red_list: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "        # -- Step 8: Define MSF hierarchy\n",
    "        try:\n",
    "            MSF_hierarchy = ['ReportPeriod', 'Cluster', 'LGA', 'FacilityName']\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining MSF_hierarchy: {str(e)}\")\n",
    "            raise  # Re-raise to trigger top-level except\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⦸ Error loading variables: {str(e)}\")\n",
    "        # Optionally assign fallback values to globals, but here we just return\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdbaef-657c-45ca-af3a-67757f53d82e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - Function: Validations & Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d564859-98d8-48d0-83a5-90f204120531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_functions():\n",
    "    \"\"\"\n",
    "    Defines and assigns global functions for styling dfs and exporting them to image, Excel, and Word formats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Declare global functions and variables\n",
    "        global outlier_red, outlier_green, export_df_to_doc_image_excel  # -- Declare styling and export functions as global\n",
    "        global filter_gap_and_check_empty_df, prepare_and_convert_df     # -- Declare DataFrame processing functions as global\n",
    "        global Pre_MSF_positives_all\n",
    "\n",
    "        # -- Step 2: Define outlier_red function\n",
    "        # -- Function: Style cells with light coral for values less than 100\n",
    "        def outlier_red(val):                                           # -- Define function to style cells red\n",
    "            \"\"\"\n",
    "            Styles cells with a light coral background and bold font for values less than 100.\n",
    "            Applies a border for consistent formatting.\n",
    "    \n",
    "            Args:\n",
    "                val: Value to evaluate (int, float, or string).\n",
    "    \n",
    "            Returns:\n",
    "                str: CSS style string if condition met.\n",
    "            \"\"\"\n",
    "            try:                                                        # -- Begin try block for error handling\n",
    "                # -- Step 2.1: Define conditions for styling\n",
    "                condition = (                                           # -- Combine all conditions for red styling\n",
    "                    ((isinstance(val, (int, float)) and val < 100) or   # -- Check if numeric and less than 100\n",
    "                     (isinstance(val, object) and val != '100' and val != ''))  # -- Check if string, not '100', and not empty\n",
    "                    or                                                  # -- OR condition for additional cases\n",
    "                    (not (isinstance(val, (int, float)) and val < 100) and  # -- Check if not numeric less than 100\n",
    "                     (isinstance(val, (int, float)) and val != 0))      # -- AND numeric not equal to 0\n",
    "                ) and (not (isinstance(val, (int, float)) and val == 0))   # -- AND ensure exclusion of numeric 0\n",
    "\n",
    "                # -- Step 2.2: Apply styling if condition is met\n",
    "                if condition:                                           # -- Evaluate combined condition\n",
    "                    return 'background-color: lightcoral; font-weight: bold; border-bottom: 0.01px solid #f3f3f3;'  # -- Return red styling for matching values\n",
    "                return None                                             # -- Return None if no styling applies\n",
    "            except Exception as e:                                      # -- Catch exceptions in outlier_red definition\n",
    "                print(f\"⦸ Error defining outlier_red: {str(e)}\")        # -- Print error message\n",
    "                return None                                             # -- Return None on error\n",
    "        \n",
    "        # -- Step 3: Define outlier_green function\n",
    "        # -- Function: Style cells with light green for values equal to 100\n",
    "        try:\n",
    "            def outlier_green(val):\n",
    "                \"\"\"\n",
    "                Styles cells with a light green background and bold font for values equal to 100.\n",
    "                Applies a border for consistent formatting.\n",
    "                \n",
    "                Args:\n",
    "                    val: Value to evaluate (int, float, or string).\n",
    "                \n",
    "                Returns:\n",
    "                    str: CSS style string if condition met, None otherwise.\n",
    "                \"\"\"\n",
    "                if (isinstance(val, (int, float)) and val == 100) or (isinstance(val, object) and val == '100' and val != ''):\n",
    "                    return 'background-color: lightgreen; font-weight: bold; border-bottom: 0.01px solid #f3f3f3;'\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining outlier_green: {str(e)}\")           # -- Print error message for outlier_green\n",
    "            outlier_green = lambda val: None                            # -- Fallback to a no-op function\n",
    "\n",
    "        # -- Step 4: Define export_df_to_doc_image_excel function\n",
    "        try:\n",
    "            def export_df_to_doc_image_excel(\n",
    "                report_name=None,\n",
    "                df_shape=None,\n",
    "                df_style=None,\n",
    "                img_file_name=None,\n",
    "                img_file_path=None,\n",
    "                doc_description=None,\n",
    "                doc_indicators_to_italicize=None,\n",
    "                doc_indicators_to_underline=None,\n",
    "                doc_file_path=doc_file_msf_outlier_docx,\n",
    "                xlm_file_path=None,\n",
    "                xlm_sheet_name=None,\n",
    "                highlight_red_list=highlight_red_list\n",
    "            ):\n",
    "                \"\"\"\n",
    "                Exports a df to an image, Excel file, and Word document with optional description for the document.\n",
    "                (Docstring truncated for brevity; see original for full details.)\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    # -- Step 4.1: Process image export\n",
    "                    if all([df_shape is not None, df_style is not None, img_file_name is not None, img_file_path is not None]):  # -- Check if all image params are provided\n",
    "                        n_rows, n_cols = df_shape.shape                 # -- Get DataFrame dimensions\n",
    "                        fig_width = max(10, n_cols * 1.5)               # -- Calculate figure width based on column count\n",
    "                        fig_height = max(5, n_rows * 1)                 # -- Calculate figure height based on row count\n",
    "\n",
    "                        styled_df = (df_style.set_table_styles([        # -- Apply table styles to DataFrame\n",
    "                            {'selector': 'th', 'props': [('max-width', '100px'), ('word-wrap', 'break-word'), ('text-align', 'right'), ('background-color', 'white'), ('padding', '2px 10px')]},\n",
    "                            {'selector': 'td', 'props': [('text-align', 'right'), ('padding', '2px 10px')]},\n",
    "                            {'selector': 'tr:nth-child(odd)', 'props': [('background-color', '#f2f2f2')]},\n",
    "                            {'selector': 'tr:nth-child(even)', 'props': [('background-color', 'white')]}\n",
    "                        ]).set_table_attributes('style=\"font-family: Calibri; font-size: 12pt; border-collapse: collapse;\"'))\n",
    "\n",
    "                        html = f\"\"\"\n",
    "                        <div style=\"display: flex; justify-content: center; align-items: center; height: 100vh;\">\n",
    "                            {styled_df.to_html()}\n",
    "                        </div>\n",
    "                        \"\"\"\n",
    "\n",
    "                        hti = Html2Image()                              # -- Initialize Html2Image for screenshot\n",
    "                        hti.output_path = img_file_path                 # -- Set output path for image\n",
    "                        image_path = os.path.join(img_file_path, img_file_name)  # -- Define full image path\n",
    "                        hti.screenshot(html_str=html, save_as=img_file_name, size=(int(fig_width * 100), int(fig_height * 100)))  # -- Generate screenshot\n",
    "\n",
    "                        img = Image.open(image_path).convert('RGBA')    # -- Open image for cropping\n",
    "                        bbox = img.getbbox()                            # -- Get bounding box for cropping\n",
    "                        if bbox:                                        # -- Check if bounding box exists\n",
    "                            cropped_img = img.crop(bbox)                # -- Crop image to content\n",
    "                            cropped_img.save(image_path, format='PNG')  # -- Save cropped image\n",
    "                    else:\n",
    "                        image_path = None                               # -- Set image_path to None if params missing\n",
    "\n",
    "                    # -- Step 4.2: Process Excel export\n",
    "                    if all([df_style is not None, xlm_file_path is not None, xlm_sheet_name is not None]):  # -- Check if all Excel params are provided\n",
    "                        xlm_sheet_name = xlm_sheet_name[:31]            # -- Truncate sheet name to 31 characters\n",
    "                        mode = 'a' if os.path.exists(xlm_file_path) else 'w'  # -- Set mode: append if file exists, write if not\n",
    "                        with pd.ExcelWriter(xlm_file_path, engine='openpyxl', mode=mode, if_sheet_exists='replace' if mode == 'a' else None) as writer:  # -- Open Excel writer\n",
    "                            df_style.to_excel(writer, sheet_name=xlm_sheet_name, index=False)  # -- Write styled DataFrame to Excel\n",
    "\n",
    "                        wb = load_workbook(xlm_file_path)               # -- Load workbook for further formatting\n",
    "                        ws = wb[xlm_sheet_name]                         # -- Select worksheet by truncated name\n",
    "\n",
    "                        for cell in ws[1]:                              # -- Process header row for formatting\n",
    "                            if cell.value and isinstance(cell.value, str):  # -- Check if cell has string value\n",
    "                                protected_map = {}                      # -- Initialize map for protected phrases\n",
    "                                for phrase in highlight_red_list or []: # -- Iterate over phrases to highlight\n",
    "                                    token = f\"@@PROTECT_{abs(hash(phrase))}@@\"  # -- Create unique token\n",
    "                                    protected_map[token] = phrase       # -- Map token to phrase\n",
    "                                    cell.value = cell.value.replace(phrase, token)  # -- Replace phrase with token\n",
    "                                cell.value = re.sub(r\"<.*?>\", \"\", cell.value)  # -- Remove HTML tags\n",
    "                                for token, phrase in protected_map.items():  # -- Restore protected phrases\n",
    "                                    cell.value = cell.value.replace(token, phrase)  # -- Replace token with phrase\n",
    "                                cell.value = cell.value.strip()         # -- Strip whitespace\n",
    "\n",
    "                        font_style = Font(name='Calibri', size=8)       # -- Define font style for cells\n",
    "                        header_font = Font(name='Calibri', size=8, bold=True)  # -- Define font style for headers\n",
    "                        header_alignment = Alignment(horizontal=\"left\", vertical=\"bottom\", wrap_text=True)  # -- Define header alignment\n",
    "\n",
    "                        for row in ws.iter_rows():                      # -- Apply font style to all cells\n",
    "                            for cell in row:\n",
    "                                cell.font = font_style\n",
    "\n",
    "                        for cell in ws[1]:                              # -- Apply header formatting\n",
    "                            cell.alignment = header_alignment\n",
    "                            cell.font = header_font\n",
    "\n",
    "                        for col in ws.iter_cols(min_col=1, max_col=4):  # -- Adjust column widths\n",
    "                            max_length = max((len(str(cell.value)) if cell.value else 0) for cell in col)  # -- Find max length\n",
    "                            ws.column_dimensions[col[0].column_letter].width = max_length  # -- Set column width\n",
    "\n",
    "                        ws.auto_filter.ref = ws.dimensions              # -- Enable auto-filter for sheet\n",
    "                        wb.save(xlm_file_path)                          # -- Save workbook\n",
    "\n",
    "                    # -- Step 4.3: Create or append to Word document\n",
    "                    if all([doc_file_path is not None, image_path is not None, doc_indicators_to_italicize is not None, doc_indicators_to_underline is not None]):  # -- Check if all doc params are provided\n",
    "                        if os.path.exists(doc_file_path):               # -- Check if document exists\n",
    "                            doc = Document(doc_file_path)               # -- Load existing document\n",
    "                        else:\n",
    "                            doc = Document()                            # -- Create new document\n",
    "\n",
    "                        style = doc.styles['Normal']                    # -- Set default style\n",
    "                        style.font.name = 'Calibri'                     # -- Set font to Calibri\n",
    "                        style.font.size = Pt(9.5)                       # -- Set font size\n",
    "\n",
    "                        for section in doc.sections:                    # -- Configure section margins\n",
    "                            section.left_margin = Inches(0.5)\n",
    "                            section.right_margin = Inches(0.5)\n",
    "                            section.top_margin = Inches(1)\n",
    "                            section.bottom_margin = Inches(1)\n",
    "\n",
    "                        if doc_description:                             # -- Add description if provided\n",
    "                            title_paragraph = doc.add_heading(report_name, level=2)  # -- Add report name as heading\n",
    "                            title_run = title_paragraph.runs[0]         # -- Get title run\n",
    "                            title_run.font.size = Pt(10)                # -- Set title font size\n",
    "                            title_run.font.color.rgb = RGBColor(0, 0, 0)  # -- Set title color\n",
    "\n",
    "                            paragraph = doc.add_paragraph()             # -- Add paragraph for description\n",
    "                            paragraph.paragraph_format.space_after = Pt(0)  # -- Remove space after paragraph\n",
    "\n",
    "                            phrases_to_bold = [                         # -- Define phrases to bold\n",
    "                                \"REPORT ONLY 2025 LIVE BIRTHS BY PPW\",\n",
    "                                \"REPORT ONLY HEI ARVs FOR 2025 LIVE BIRTHS BY PPW\",\n",
    "                                \"REPORT ONLY EID SAMPLE COLLECTION FOR 2025 LIVE BIRTHS BY PPW\",\n",
    "                                \"REPORT ONLY EID PCR RESULTS FOR 2025 LIVE BIRTHS BY PPW\",\n",
    "                                \"Report Name:\", \"should not be greater than\",\n",
    "                                \"should not be lesser than\", \"should not be equal to\",\n",
    "                                \"should be greater than\", \"should be lesser than\",\n",
    "                                \"should be equal to\", \"plus\", \"Note\", \"OR\"\n",
    "                            ]\n",
    "                            all_phrases = phrases_to_bold + (doc_indicators_to_italicize or []) + (doc_indicators_to_underline or [])  # -- Combine all phrases\n",
    "                            pattern = r'|'.join(re.escape(phrase) for phrase in all_phrases)  # -- Create regex pattern\n",
    "                            matches = list(re.finditer(pattern, doc_description))  # -- Find matches in description\n",
    "\n",
    "                            last_index = 0                              # -- Track last processed index\n",
    "                            for match in matches:                       # -- Process each match\n",
    "                                start, end = match.start(), match.end() # -- Get match boundaries\n",
    "                                paragraph.add_run(doc_description[last_index:start])  # -- Add text before match\n",
    "                                run = paragraph.add_run(doc_description[start:end])  # -- Add matched text\n",
    "                                if match.group(0) in phrases_to_bold:   # -- Apply bold if in bold list\n",
    "                                    run.bold = True\n",
    "                                if match.group(0) in doc_indicators_to_italicize:  # -- Apply italic if in italicize list\n",
    "                                    run.italic = True\n",
    "                                if match.group(0) in doc_indicators_to_underline:  # -- Apply underline if in underline list\n",
    "                                    run.underline = True\n",
    "                                last_index = end                            # -- Update last index\n",
    "\n",
    "                            paragraph.add_run(doc_description[last_index:])  # -- Add remaining text\n",
    "\n",
    "                        doc.add_picture(image_path, width=Inches(7))    # -- Add image to document\n",
    "\n",
    "                        section = doc.sections[-1]                      # -- Get last section for footer\n",
    "                        footer = section.footer.paragraphs[0]           # -- Access footer paragraph\n",
    "                        footer.text = \"This is an auto-generated report. Ensure all data is reviewed before any update is made.\"  # -- Set footer text\n",
    "                        footer.runs[0].font.size = Pt(7.5)             # -- Set footer font size\n",
    "                        footer.runs[0].font.color.rgb = RGBColor(100, 100, 100)  # -- Set footer color\n",
    "\n",
    "                        doc.save(doc_file_path)                         # -- Save document\n",
    "\n",
    "                    # -- Step 4.4: Generate and print success messages\n",
    "                    if all([report_name, img_file_name, img_file_path, xlm_file_path, xlm_sheet_name]):  # -- Check if all params for success message are provided\n",
    "                        img_file_path_name = os.path.basename(img_file_path)  # -- Get base image path name\n",
    "                        xlm_file_path_name = os.path.basename(xlm_file_path)  # -- Get base Excel path name\n",
    "                        image_success_print = rf\"IMG: '{img_file_name}' in C:\\file_path\\{img_file_path_name}\"  # -- Format image success message\n",
    "                        excel_success_print = rf\"XLS: '{xlm_sheet_name}' in C:\\file_path\\{xlm_file_path_name}\"  # -- Format Excel success message\n",
    "                        \n",
    "                        messages = [image_success_print, excel_success_print]  # -- Initialize success messages list\n",
    "                        if doc_description:                             # -- Check if document description exists\n",
    "                            doc_file_path_name = os.path.basename(doc_file_path)  # -- Get base document path name\n",
    "                            doc_success_print = rf\"DOC: '{report_name}' in C:\\file_path\\{doc_file_path_name}\"  # -- Format document success message\n",
    "                            messages.append(doc_success_print)          # -- Add document message to list\n",
    "\n",
    "                        separator_line = '-' * max(len(msg) for msg in messages)  # -- Create separator line based on longest message\n",
    "\n",
    "                        print(f\"✔️ {report_name}\")                      # -- Print report name with checkmark\n",
    "                        print(separator_line)                           # -- Print separator line\n",
    "                        print('\\n'.join(messages))                      # -- Print success messages\n",
    "                        print(separator_line)                           # -- Print separator line\n",
    "\n",
    "                    return image_path                                   # -- Return image path\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⦸ Error in export_df_to_doc_image_excel: {str(e)}\")  # -- Print error message\n",
    "                    return None                                         # -- Return None on error\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining export_df_to_doc_image_excel: {str(e)}\")  # -- Print error message for function definition\n",
    "            export_df_to_doc_image_excel = lambda *args, **kwargs: None  # -- Fallback to a no-op function\n",
    "\n",
    "        # -- Step 5: Define prepare_and_convert_df function\n",
    "        def prepare_and_convert_df(DHIS2_data_key=None, hierarchy_columns=None, data_columns=None):  # -- Define function to prepare and convert DataFrame\n",
    "            \"\"\"\n",
    "            Prepare and convert a DataFrame from DHIS2_data with available columns, applying sorting,\n",
    "            default values for missing data columns, and type conversions.\n",
    "\n",
    "            Args:\n",
    "                DHIS2_data_key (str): The key to look up the DHIS2 dataset.\n",
    "                hierarchy_columns (List[str] or None): List of hierarchy columns to include.\n",
    "                data_columns (List[str]): List of desired data columns.\n",
    "\n",
    "            Returns:\n",
    "                Optional[pd.DataFrame]: Prepared DataFrame or None if error occurs.\n",
    "            \"\"\"\n",
    "            try:                                                        # -- Begin try block for preparation function\n",
    "                if DHIS2_data_key not in DHIS2_data:                    # -- Check if DHIS2 key exists\n",
    "                    print(f\"✋🏿 Error: '{DHIS2_data_key}' not found in DHIS2_data. Report not processed.\")  # -- Print error message\n",
    "                    return None                                         # -- Return None if key missing\n",
    "\n",
    "                df_raw = DHIS2_data[DHIS2_data_key]                     # -- Get raw DataFrame from DHIS2 data\n",
    "\n",
    "                if hierarchy_columns is None:                           # -- Check if hierarchy columns are provided\n",
    "                    hierarchy_columns = []                              # -- Default to empty list if None\n",
    "\n",
    "                available_columns = [col for col in data_columns if col in df_raw.columns]  # -- Filter available data columns\n",
    "                if not available_columns:                               # -- Check if any requested columns are available\n",
    "                    print(f\"✋🏿 Warning: None of the requested columns found in '{DHIS2_data_key}'.\")  # -- Print warning\n",
    "                    # No return here to allow empty DataFrame with hierarchy\n",
    "\n",
    "                df = df_raw[hierarchy_columns + available_columns].copy()  # -- Copy selected columns\n",
    "                df.sort_values(by=hierarchy_columns, inplace=True, ignore_index=True)  # -- Sort by hierarchy columns\n",
    "\n",
    "                for col in available_columns:                           # -- Iterate over available columns\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)  # -- Convert to numeric, fill NA with 0, cast to int\n",
    "\n",
    "                for col in data_columns:                                # -- Iterate over desired columns\n",
    "                    if col not in df.columns:                           # -- Check if column is missing\n",
    "                        df[col] = 0                                     # -- Add missing column with default 0\n",
    "\n",
    "                df = df[hierarchy_columns + data_columns]               # -- Reorder columns to match input order\n",
    "\n",
    "                return df                                               # -- Return prepared DataFrame\n",
    "\n",
    "            except Exception as e:                                      # -- Catch exceptions in preparation function\n",
    "                print(f\"⦸ Error preparing and converting df for '{DHIS2_data_key}': {str(e)}\")  # -- Print error message\n",
    "                return None                                             # -- Return None on failure\n",
    "        \n",
    "        # -- Step 6: Define filter_gap_and_check_empty_df function\n",
    "        # -- Function: Filter gap and check empty df\n",
    "        try:\n",
    "            def filter_gap_and_check_empty_df(\n",
    "                df=None, msg=None, opNonZero=None, opNeg=None, opPos=None, \n",
    "                opNonPos=None, opNonNeg=None, opZero=None, opLT100=None):\n",
    "                \"\"\"\n",
    "                Filters a df based on column-specific conditions and handles empty results.\n",
    "                (Docstring truncated for brevity; see original for full details.)\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    if df is None or df.empty:                          # -- Check if input DataFrame is invalid\n",
    "                        raise ValueError(\"Input df is None or empty\")   # -- Raise error if None or empty\n",
    "                    if not msg:                                         # -- Check if message is provided\n",
    "                        raise ValueError(\"No message provided for empty result\")  # -- Raise error if missing\n",
    "\n",
    "                    operator_map = {                                    # -- Define operator mapping for conditions\n",
    "                        'opNonZero': lambda x: x != 0,                  # -- Non-zero condition\n",
    "                        'opNeg': lambda x: x < 0,                       # -- Negative condition\n",
    "                        'opPos': lambda x: x > 0,                       # -- Positive condition\n",
    "                        'opZero': lambda x: x == 0,                     # -- Zero condition\n",
    "                        'opLT100': lambda x: x < 100                    # -- Less than 100 condition\n",
    "                    }\n",
    "\n",
    "                    conditions = []                                     # -- Initialize conditions list\n",
    "                    for arg, cols in {                                  # -- Iterate over operator arguments\n",
    "                        'opNonZero': opNonZero, 'opNeg': opNeg, 'opPos': opPos, \n",
    "                        'opNonPos': opNonPos, 'opNonNeg': opNonNeg, 'opZero': opZero, 'opLT100': opLT100\n",
    "                    }.items():\n",
    "                        if cols:                                        # -- Check if columns are provided for operator\n",
    "                            for col in cols:                            # -- Iterate over columns\n",
    "                                if col not in df.columns:               # -- Check if column exists\n",
    "                                    raise ValueError(f\"Column '{col}' not found in df\")  # -- Raise error if missing\n",
    "                                numeric_series = pd.to_numeric(df[col], errors='coerce').fillna(0)  # -- Convert to numeric\n",
    "                                conditions.append(operator_map[arg](numeric_series))  # -- Apply operator condition\n",
    "\n",
    "                    if not conditions:                              # -- Check if any conditions were added\n",
    "                        print(\"No filtering conditions provided\")    # -- Print warning\n",
    "                        return df                                    # -- Return original DataFrame\n",
    "\n",
    "                    combined_condition = pd.DataFrame(conditions).T.any(axis=1)  # -- Combine conditions with OR logic\n",
    "                    df_filtered = df[combined_condition]            # -- Filter DataFrame\n",
    "\n",
    "                    if df_filtered.empty:                            # -- Check if filtered DataFrame is empty\n",
    "                        print(\"✋🏿Checked:\")                        # -- Print check message\n",
    "                        print(\"-\" * len(msg))                       # -- Print separator line\n",
    "                        print(msg)                                  # -- Print empty result message\n",
    "                        print(\"-\" * len(msg))                       # -- Print separator line\n",
    "                        return None                                 # -- Return None for empty result\n",
    "\n",
    "                    return df_filtered                              # -- Return filtered DataFrame\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⦸ Error filtering gaps and checking empty df: {str(e)}\")  # -- Print error message\n",
    "                    return None                                     # -- Return None on error\n",
    "        except Exception as e:\n",
    "            print(f\"⦸ Error defining filter_gap_and_check_empty_df: {str(e)}\")  # -- Print error message for function definition\n",
    "            filter_gap_and_check_empty_df = lambda *args, **kwargs: None  # -- Fallback to a no-op function\n",
    "\n",
    "         # -- Step 7: Constants Initialization for Positive Data Processing\n",
    "        # -- Define column lists for HTS, AGYW, PMTCT, and KP datasets\n",
    "        HTS_cols = {\n",
    "            \"positive\": [\n",
    "                \"Number of people who tested HIV positive and received results (Inpatient)\",  # -- Inpatient positive results\n",
    "                \"Number of people who tested HIV positive and received results (Outpatient)\",  # -- Outpatient positive results\n",
    "                \"Number of people who tested HIV positive and received results (Standalone)\",  # -- Standalone positive results\n",
    "                \"Number of people who tested HIV positive and received results (Community)\"  # -- Community positive results\n",
    "            ],\n",
    "            \"known_positive\": [\n",
    "                \"Total number of people tested HIV positive that were identified as known positive during post-test counselling.(Inpatient)\",  # -- Known positive inpatient\n",
    "                \"Total number of people tested HIV positive that were identified as known positive during post-test counselling.(Outpatient)\",  # -- Known positive outpatient\n",
    "                \"Total number of people tested HIV positive that were identified as known positive during post-test counselling.(Standalone)\",  # -- Known positive standalone\n",
    "                \"Total number of people tested HIV positive that were identified as known positive during post-test counselling.(Community)\"  # -- Known positive community\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        AGYW_cols = [\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Community)\",  # -- AGYW community positive\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Walk-In)\"  # -- AGYW walk-in positive\n",
    "        ]\n",
    "\n",
    "        PMTCT_col = [\"Number of pregnant women tested HIV positive\"]  # -- PMTCT positive results\n",
    "\n",
    "        KP_cols = [\n",
    "            \"HTS-3a Number of MSM that have received an HIV test during the reporting period in KP-specific programs and received HIV Positive results\",  # -- MSM positive results\n",
    "            \"HTS-3b Number of TG that have received an HIV test during the reporting period in KP-specific programs and HIV positive results\",  # -- TG positive results\n",
    "            \"HTS-3c Number of sex workers that have received an HIV test during the reporting period in KP-specific programs and received HIV-positive results\",  # -- Sex workers positive\n",
    "            \"HTS-3d Number of people who inject drugs (PWID) that have received an HIV test during the reporting period in KP-specific programs and received HIV positive results\",  # -- PWID positive\n",
    "            \"HTS-3e Number of other vulnerable populations (OVP) that have received an HIV test during the reporting period and received HIV-positive results\",  # -- OVP positive\n",
    "            \"HTS-3f Number of people in prisons and other closed settings that have received an HIV test during the reporting period and received HIV-positive results\"  # -- Prison positive\n",
    "        ]\n",
    "\n",
    "        # -- Step 8: Prepare and process HTS data\n",
    "        Pre_HTS_MSF_positive = prepare_and_convert_df('HTS_MSF', MSF_hierarchy, HTS_cols[\"positive\"] + HTS_cols[\"known_positive\"])  # -- Prepare HTS DataFrame\n",
    "        Pre_HTS_MSF_positive[\"HTS total tested - positive\"] = Pre_HTS_MSF_positive[HTS_cols[\"positive\"]].sum(axis=1)  # -- Calculate total positive\n",
    "        Pre_HTS_MSF_positive[\"HTS total tested - previously known positive\"] = Pre_HTS_MSF_positive[HTS_cols[\"known_positive\"]].sum(axis=1)  # -- Calculate known positive\n",
    "        Pre_HTS_MSF_positive[\"HTS total tested - new positive (excluding previously known)\"] = np.where(\n",
    "            Pre_HTS_MSF_positive[\"HTS total tested - previously known positive\"] > 0,  # -- Check if known positive exists\n",
    "            Pre_HTS_MSF_positive[\"HTS total tested - positive\"] - Pre_HTS_MSF_positive[\"HTS total tested - previously known positive\"],  # -- Calculate new positive\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # -- Step 9: Prepare AGYW data\n",
    "        Pre_AGYW_MSF_positive = prepare_and_convert_df('AGYW_MSF', MSF_hierarchy, AGYW_cols)  # -- Prepare AGYW DataFrame\n",
    "        Pre_AGYW_MSF_positive[\"AGYW total tested - new positive\"] = Pre_AGYW_MSF_positive[AGYW_cols].sum(axis=1)  # -- Calculate total AGYW positive\n",
    "\n",
    "        # -- Step 10: Prepare PMTCT data\n",
    "        Pre_PMTCT_MSF_positive = prepare_and_convert_df('PMTCT_MSF', MSF_hierarchy, PMTCT_col)  # -- Prepare PMTCT DataFrame\n",
    "        Pre_PMTCT_MSF_positive.rename(columns={PMTCT_col[0]: \"PMTCT total tested - new positive\"}, inplace=True)  # -- Rename PMTCT column\n",
    "\n",
    "        # -- Step 11: Prepare KP data\n",
    "        Pre_KP_MSF_positive = prepare_and_convert_df('KP_Prev_MSF', MSF_hierarchy, KP_cols)  # -- Prepare KP DataFrame\n",
    "        Pre_KP_MSF_positive[\"KP_Prev total tested - new positive\"] = Pre_KP_MSF_positive[KP_cols].sum(axis=1)  # -- Calculate total KP positive\n",
    "\n",
    "        # -- Step 12: Merge all DataFrames sequentially\n",
    "        base_cols = [\"ReportPeriod\", \"Cluster\", \"LGA\", \"FacilityName\"]  # -- Define base columns for merging\n",
    "        Pre_MSF_positives_all = DHIS2_data[\"Report_Rate_Facility\"][base_cols].merge(\n",
    "            Pre_HTS_MSF_positive[base_cols + [\"HTS total tested - new positive (excluding previously known)\"]],  # -- Merge HTS data\n",
    "            on=base_cols, how=\"left\"\n",
    "        ).merge(\n",
    "            Pre_AGYW_MSF_positive[base_cols + [\"AGYW total tested - new positive\"]],  # -- Merge AGYW data\n",
    "            on=base_cols, how=\"left\"\n",
    "        ).merge(\n",
    "            Pre_PMTCT_MSF_positive[base_cols + [\"PMTCT total tested - new positive\"]],  # -- Merge PMTCT data\n",
    "            on=base_cols, how=\"left\"\n",
    "        ).merge(\n",
    "            Pre_KP_MSF_positive[base_cols + [\"KP_Prev total tested - new positive\"]],  # -- Merge KP data\n",
    "            on=base_cols, how=\"left\"\n",
    "        )\n",
    "\n",
    "        # -- Step 13: Ensure numeric types & fill NaNs\n",
    "        for col in [\n",
    "            \"HTS total tested - new positive (excluding previously known)\",  # -- HTS new positive column\n",
    "            \"AGYW total tested - new positive\",                             # -- AGYW new positive column\n",
    "            \"PMTCT total tested - new positive\",                            # -- PMTCT new positive column\n",
    "            \"KP_Prev total tested - new positive\"                           # -- KP new positive column\n",
    "        ]:\n",
    "            Pre_MSF_positives_all[col] = pd.to_numeric(Pre_MSF_positives_all[col], errors='coerce').fillna(0).astype(int)  # -- Convert to int, fill NaNs with 0\n",
    "            Pre_MSF_positives_all[\"Total new positive\"] = Pre_MSF_positives_all[[  # -- Sum total new positive\n",
    "                \"HTS total tested - new positive (excluding previously known)\",\n",
    "                \"AGYW total tested - new positive\",\n",
    "                \"PMTCT total tested - new positive\",\n",
    "                \"KP_Prev total tested - new positive\"\n",
    "            ]].sum(axis=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⦸ Error in load_functions: {str(e)}\")                    # -- Print error message for load_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572cb49-2d9f-49df-bcf4-f1f51e14f424",
   "metadata": {},
   "source": [
    "#### - Function: Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d146cdc-5e76-4cca-886e-da0d5a9da049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Global variable to store DHIS2_data\n",
    "def fetch_dhis2_data_interactive_jupyter_mode():\n",
    "    \"\"\"\n",
    "    Interactive function to collect user inputs and fetch/process DHIS2 data in a Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        dict: DHIS2_data fetched from the DHIS2 server (accessible globally after submission)\n",
    "    \"\"\"\n",
    "    global DHIS2_data  # -- Declare DHIS2_data as global to modify it within handlers\n",
    "    \n",
    "    # -- Step 1: Define constants\n",
    "    separator_line = '-' * 43  # -- Define a static separator line of 43 dashes for formatting\n",
    "    output = widgets.Output()  # -- Output area for displaying results\n",
    "\n",
    "    # -- Step 2: Create widgets for collecting credentials\n",
    "    username_input = widgets.Text(description=\"Username:\", placeholder=\"Enter IHVN DHIS2 username\")\n",
    "    password_input = widgets.Password(description=\"Passkey:\", placeholder=\"Enter IHVN DHIS2 password\")\n",
    "    submit_credentials = widgets.Button(description=\"Submit\")\n",
    "    credentials_box = widgets.VBox([username_input, password_input, submit_credentials])\n",
    "\n",
    "    # -- Step 3: Create widgets for collecting periods\n",
    "    start_period_input = widgets.Text(description=\"Start Period:\", placeholder=\"Enter report period (YYYYMM, e.g., 202501)\")\n",
    "    end_period_input = widgets.Text(description=\"End Period:\", placeholder=\"Enter report period (YYYYMM, e.g., 202512)\")\n",
    "    submit_periods = widgets.Button(description=\"Submit\")\n",
    "    periods_box = widgets.VBox([start_period_input, end_period_input, submit_periods])\n",
    "\n",
    "    # -- Step 4: Store collected inputs\n",
    "    credentials = [None, None]  # -- To store username and password\n",
    "    periods = [None, None]      # -- To store start_period and end_period\n",
    "\n",
    "    # -- Step 5: Define formatting functions\n",
    "    def format_credentials(username, password):\n",
    "        \"\"\"Format login credentials for display with masked password.\"\"\"\n",
    "        username_line = f\"{'Username: ':<{43 - len(username)}}{username}\"\n",
    "        password_line = f\"{'Passkey: ':<{43 - len(password)}}{'*' * len(password)}\"\n",
    "        return f\"{username_line}\\n{password_line}\"\n",
    "\n",
    "    def format_report_period(start_period, end_period):\n",
    "        \"\"\"Format report period dates for display.\"\"\"\n",
    "        start_period_line = f\"{'Period Start Date: ':<{43 - len(start_period)}}{start_period}\"\n",
    "        end_period_line = f\"{'Period End Date: ':<{43 - len(end_period)}}{end_period}\"\n",
    "        return f\"{start_period_line}\\n{end_period_line}\"\n",
    "\n",
    "    def display_information(credentials_display, report_display):\n",
    "        \"\"\"Display all collected information in a formatted way.\"\"\"\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"Enter IHVN DHIS2 login credentials:\")\n",
    "            print(separator_line)\n",
    "            print(credentials_display)\n",
    "            print(separator_line)\n",
    "            print()\n",
    "            print('Enter report period (YYYYMM, e.g., 202501):')\n",
    "            print(separator_line)\n",
    "            print(report_display)\n",
    "            print(separator_line)\n",
    "            print()\n",
    "\n",
    "    # -- Step 6: Define button handlers\n",
    "    def on_submit_credentials(b):\n",
    "        \"\"\"Handle submission of credentials.\"\"\"\n",
    "        credentials[0] = username_input.value\n",
    "        credentials[1] = password_input.value\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"Enter IHVN DHIS2 login credentials:\")\n",
    "            print(separator_line)\n",
    "            print(format_credentials(credentials[0], credentials[1]))\n",
    "            print(separator_line)\n",
    "            print()\n",
    "            print('Enter report period (YYYYMM, e.g., 202501):')\n",
    "            print(separator_line)\n",
    "            display(periods_box)\n",
    "\n",
    "    def on_submit_periods(b):\n",
    "        \"\"\"Handle submission of periods and fetch data.\"\"\"\n",
    "        global DHIS2_data\n",
    "        periods[0] = start_period_input.value\n",
    "        periods[1] = end_period_input.value\n",
    "\n",
    "        # Display formatted info\n",
    "        credentials_display = format_credentials(credentials[0], credentials[1])\n",
    "        report_display = format_report_period(periods[0], periods[1])\n",
    "        display_information(credentials_display, report_display)\n",
    "\n",
    "        # -- Fetch and process DHIS2 data (placeholder)\n",
    "        with output:\n",
    "            DHIS2_data = fetch_and_process_DHIS2_data(credentials[0], credentials[1], periods[0], periods[1])\n",
    "            load_variables()\n",
    "            load_functions()\n",
    "\n",
    "            # -- Optional LGA Filter Widget Setup\n",
    "            if \"Report_Rate_LGA\" in DHIS2_data and \"LGA\" in DHIS2_data[\"Report_Rate_LGA\"].columns:\n",
    "                available_lgas = sorted(DHIS2_data[\"Report_Rate_LGA\"][\"LGA\"].dropna().unique())\n",
    "\n",
    "                lga_filter_widget = widgets.SelectMultiple(\n",
    "                    options=available_lgas,\n",
    "                    description=\"Select LGA:\",\n",
    "                    #layout=widgets.Layout(width='300px', height='150px'),\n",
    "                    rows=6\n",
    "                )\n",
    "\n",
    "                apply_filter_button = widgets.Button(description=\"Apply report level\")\n",
    "\n",
    "                def on_apply_filter_clicked(b):\n",
    "                    selected_lgas = list(lga_filter_widget.value)\n",
    "                    with output:\n",
    "                        if not selected_lgas:\n",
    "                            print(\"✔️ State level data ready\")\n",
    "                        else:\n",
    "                            for key, df in DHIS2_data.items():\n",
    "                                if isinstance(df, pd.DataFrame) and \"LGA\" in df.columns:\n",
    "                                    DHIS2_data[key] = df[df[\"LGA\"].isin(selected_lgas)].copy()\n",
    "                            print(f\"✔️ LGA level data ready for {selected_lgas}\")\n",
    "\n",
    "                apply_filter_button.on_click(on_apply_filter_clicked)\n",
    "\n",
    "                print(f\"\\nOptional: Select report level - LGA\")\n",
    "                print(separator_line)\n",
    "                display(widgets.VBox([\n",
    "                    lga_filter_widget,\n",
    "                    apply_filter_button\n",
    "                ]))\n",
    "                print(separator_line)\n",
    "                \n",
    "    # -- Step 7: Link buttons to handlers\n",
    "    submit_credentials.on_click(on_submit_credentials)\n",
    "    submit_periods.on_click(on_submit_periods)\n",
    "\n",
    "    # -- Step 8: Display the initial interface\n",
    "    with output:\n",
    "        print(\"Enter IHVN DHIS2 login credentials:\")\n",
    "        print(separator_line)\n",
    "        display(credentials_box)\n",
    "    display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e50f1-1bbf-40c8-926b-44cf8d40c9af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MSF reporting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790c3c4-0e96-42b1-8805-cdfa10a3c9d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - Reporting rate: LGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86fa7fd8-3a51-4d37-afaa-22dc1f21a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process LGA report rate gap\n",
    "def process_lga_report_rate_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process LGA report rate gap, exporting results as image and Excel files.\n",
    "    Caches the styled df and df shape for faster display in subsequent calls.\n",
    "    Reprocesses if the df shape changes.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the styled df for LGAs with gap.\n",
    "            Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        MSF_report_rate_columns = [                           # -- Define list of MSF report rate columns\n",
    "            'AGYW Monthly Summary Form - Reporting rate',\n",
    "            'ART MSF - Reporting rate',\n",
    "            'Care & Support MSF - Reporting rate',\n",
    "            'HTS Summary Form - Reporting rate',\n",
    "            'NSP Summary Form - Reporting rate',\n",
    "            'PMTCT MSF - Reporting rate',\n",
    "            'Prevention Summary Form - Reporting rate'\n",
    "        ]\n",
    "        MSF_report_rate_msg = \"No ANSO report rate gap found\"  # -- Define message for no gaps\n",
    "        report_name = \"ANSO MSF report rate\"                  # -- Define report name\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_Report_Rate_LGA = prepare_and_convert_df(          # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='Report_Rate_LGA',                 # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,                  # -- Use MSF hierarchy columns\n",
    "            data_columns=MSF_report_rate_columns              # -- Include specified report rate columns\n",
    "        )\n",
    "        if df_Report_Rate_LGA is None:                        # -- Check if data preparation failed or DataFrame is empty\n",
    "            return                                            # -- Exit function if no data\n",
    "\n",
    "        # -- Step 3: Set export variables\n",
    "        report_month = df_Report_Rate_LGA['ReportPeriod'].iloc[0]  # -- Extract report month from DataFrame\n",
    "        report_sheet_name = \"All LGAs\"                        # -- Define Excel sheet name\n",
    "        report_image_name = f\"{report_month}_{report_name}.png\"  # -- Define image file name\n",
    "\n",
    "        # -- Step 4: Check and display cached styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            if hasattr(process_lga_report_rate_gap, 'cached_style'):  # -- Check if cached styled DataFrame exists\n",
    "                cached_shape = getattr(process_lga_report_rate_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_Report_Rate_LGA.shape      # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:             # -- Compare shapes\n",
    "                    cached_display_name = f\"✔️ Displaying {report_name} \"  # -- Define display message\n",
    "                    print(\"-\" * len(cached_display_name))     # -- Print separator line\n",
    "                    print(cached_display_name)                # -- Print display message\n",
    "                    print(\"-\" * len(cached_display_name))     # -- Print separator line\n",
    "                    display(process_lga_report_rate_gap.cached_style)  # -- Display cached styled DataFrame\n",
    "                    return                                    # -- Exit function\n",
    "\n",
    "        # -- Step 5: Filter for gaps\n",
    "        df_Report_Rate_LGA_gap = filter_gap_and_check_empty_df(  # -- Filter DataFrame for gaps\n",
    "            df=df_Report_Rate_LGA,                            # -- Input DataFrame\n",
    "            msg=MSF_report_rate_msg,                          # -- Message for empty result\n",
    "            opNonZero=None,                                   # -- No non-zero filter\n",
    "            opNeg=None,                                       # -- No negative filter\n",
    "            opPos=None,                                       # -- No positive filter\n",
    "            opZero=None,                                      # -- No zero filter\n",
    "            opLT100=MSF_report_rate_columns                   # -- Filter for values less than 100\n",
    "        )\n",
    "        if df_Report_Rate_LGA_gap is None:                    # -- Check if no gaps found\n",
    "            if hasattr(process_lga_report_rate_gap, 'cached_style'):  # -- Check if cache exists\n",
    "                del process_lga_report_rate_gap.cached_style  # -- Clear cached style\n",
    "            if hasattr(process_lga_report_rate_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "                del process_lga_report_rate_gap.cached_shape  # -- Clear cached shape\n",
    "            return                                            # -- Exit function\n",
    "\n",
    "        # -- Step 6: Style the DataFrame\n",
    "        df_Report_Rate_LGA_style = (                          # -- Apply styling to filtered DataFrame\n",
    "            df_Report_Rate_LGA_gap.style                      # -- Start with DataFrame style object\n",
    "            .hide(axis='index')                               # -- Hide index column\n",
    "            .map(outlier_red, subset=MSF_report_rate_columns)  # -- Highlight outliers in red for report rate columns\n",
    "            .map(outlier_green)                               # -- Apply green outlier styling (assumed general application)\n",
    "        )\n",
    "\n",
    "        # -- Step 7: Cache styled DataFrame and shape\n",
    "        process_lga_report_rate_gap.cached_style = df_Report_Rate_LGA_style  # -- Cache styled DataFrame\n",
    "        process_lga_report_rate_gap.cached_shape = df_Report_Rate_LGA.shape  # -- Cache unfiltered DataFrame shape\n",
    "\n",
    "        # -- Step 8: Export results\n",
    "        export_df_to_doc_image_excel(                         # -- Export DataFrame to image and Excel formats\n",
    "            report_name=report_name,                          # -- Pass report name\n",
    "            df_shape=df_Report_Rate_LGA_gap,                  # -- Pass filtered DataFrame for shape\n",
    "            df_style=df_Report_Rate_LGA_style,                # -- Pass styled DataFrame\n",
    "            img_file_name=report_image_name,                  # -- Pass image file name\n",
    "            img_file_path=sub_folder2_image_file_report_rate, # -- Pass image file path\n",
    "            doc_description=None,                             # -- No document description (not used)\n",
    "            doc_indicators_to_italicize=None,                 # -- No indicators to italicize (not used)\n",
    "            doc_indicators_to_underline=None,                 # -- No indicators to underline (not used)\n",
    "            xlm_file_path=doc_file_report_rate_xlsx,          # -- Pass Excel file path\n",
    "            xlm_sheet_name=report_sheet_name                  # -- Pass Excel sheet name\n",
    "        )\n",
    "\n",
    "        # -- Step 9: Optionally display styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            display(df_Report_Rate_LGA_style)                 # -- Display styled DataFrame\n",
    "\n",
    "    except Exception as e:                                    # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing LGA report rate gap: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_lga_report_rate_gap, 'cached_style'):  # -- Check if cache exists\n",
    "            del process_lga_report_rate_gap.cached_style      # -- Clear cached style\n",
    "        if hasattr(process_lga_report_rate_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "            del process_lga_report_rate_gap.cached_shape      # -- Clear cached shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7db6c-270a-4af9-bf08-a0d12778cdfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - Reporting rate: Facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787a3fbc-065c-46b1-8158-17d660f1b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process facility report rate gap\n",
    "def process_facility_report_rate_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process facility report rate gaps for each LGA, exporting results as images and Excel files.\n",
    "    Caches styled dfs for each LGA and displays them on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the styled df images for each LGA with gaps.\n",
    "            Defaults to None (will treat as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        MSF_report_rate_columns = [                           # -- Define list of MSF report rate columns\n",
    "            'AGYW Monthly Summary Form - Reporting rate',\n",
    "            'ART MSF - Reporting rate',\n",
    "            'Care & Support MSF - Reporting rate',\n",
    "            'HTS Summary Form - Reporting rate',\n",
    "            'NSP Summary Form - Reporting rate',\n",
    "            'PMTCT MSF - Reporting rate',\n",
    "            'Prevention Summary Form - Reporting rate'\n",
    "        ]\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_Report_Rate_Facility = prepare_and_convert_df(     # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='Report_Rate_Facility',            # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,                  # -- Use MSF hierarchy columns\n",
    "            data_columns=MSF_report_rate_columns              # -- Include specified report rate columns\n",
    "        )\n",
    "        if df_Report_Rate_Facility is None:                   # -- Check if data preparation failed or DataFrame is empty\n",
    "            return                                            # -- Exit function if no data\n",
    "\n",
    "        # -- Step 3: Check and display cached styled DataFrames\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            if hasattr(process_facility_report_rate_gap, 'cached_styles'):  # -- Check if cached styled DataFrames exist\n",
    "                cached_shape = getattr(process_facility_report_rate_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_Report_Rate_Facility.shape  # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:             # -- Compare shapes\n",
    "                    for lga, style in process_facility_report_rate_gap.cached_styles.items():  # -- Iterate over cached styles\n",
    "                        cached_display_name = f\"✔️ Displaying {lga} facility report rate gap \"  # -- Define display message for LGA\n",
    "                        print(\"-\" * len(cached_display_name)) # -- Print separator line\n",
    "                        print(cached_display_name)            # -- Print display message\n",
    "                        print(\"-\" * len(cached_display_name)) # -- Print separator line\n",
    "                        display(style)                        # -- Display cached styled DataFrame for LGA\n",
    "                    return                                    # -- Exit function\n",
    "\n",
    "        # -- Step 4: Initialize cache\n",
    "        if not hasattr(process_facility_report_rate_gap, 'cached_styles'):  # -- Check if cache attribute exists\n",
    "            process_facility_report_rate_gap.cached_styles = {}  # -- Initialize dictionary to store styled DataFrames per LGA\n",
    "\n",
    "        # -- Step 5: Identify unique LGAs\n",
    "        lga_list = pd.Series(df_Report_Rate_Facility['LGA'].unique())  # -- Extract unique LGA names as a Series\n",
    "\n",
    "        # -- Step 6: Process each LGA for report rate gaps\n",
    "        for current_lga in lga_list:                          # -- Iterate over each unique LGA\n",
    "            # -- Step 6.1: Filter DataFrame for current LGA\n",
    "            lga_filtered = df_Report_Rate_Facility[df_Report_Rate_Facility['LGA'] == current_lga]  # -- Filter DataFrame to current LGA\n",
    "\n",
    "            MSF_report_rate_msg = f\"No {current_lga} report rate gap found\"  # -- Define message for no gaps\n",
    "\n",
    "            # -- Step 6.2: Apply filtering for gaps\n",
    "            lga_filtered_gap = filter_gap_and_check_empty_df(  # -- Filter LGA-specific subset for gaps\n",
    "                df=lga_filtered,                              # -- Input LGA-filtered DataFrame\n",
    "                msg=MSF_report_rate_msg,                      # -- Message for empty result\n",
    "                opNonZero=None,                               # -- No non-zero filter\n",
    "                opNeg=None,                                   # -- No negative filter\n",
    "                opPos=None,                                   # -- No positive filter\n",
    "                opZero=None,                                  # -- No zero filter\n",
    "                opLT100=MSF_report_rate_columns               # -- Filter for values less than 100\n",
    "            )\n",
    "\n",
    "            if lga_filtered_gap is None:                      # -- Check if no gaps found for this LGA\n",
    "                if current_lga in process_facility_report_rate_gap.cached_styles:  # -- Check if LGA is in cache\n",
    "                    del process_facility_report_rate_gap.cached_styles[current_lga]  # -- Remove LGA from cache\n",
    "                continue                                      # -- Skip to next LGA\n",
    "\n",
    "            # -- Step 6.3: Style the DataFrame\n",
    "            lga_filtered_style = (                            # -- Apply styling to filtered LGA DataFrame\n",
    "                lga_filtered_gap.style                        # -- Start with DataFrame style object\n",
    "                .hide(axis='index')                           # -- Hide index column\n",
    "                .map(outlier_red, subset=MSF_report_rate_columns)  # -- Highlight outliers in red for report rate columns\n",
    "                .map(outlier_green)                           # -- Apply green outlier styling (assumed general application)\n",
    "            )\n",
    "\n",
    "            # -- Step 6.4: Cache the styled DataFrame for this LGA\n",
    "            process_facility_report_rate_gap.cached_styles[current_lga] = lga_filtered_style  # -- Store styled DataFrame in cache\n",
    "\n",
    "            # -- Step 6.5: Define export variables\n",
    "            report_name = f\"{current_lga} Facility Report Rate Gap\"  # -- Define report name for current LGA\n",
    "            report_image_name = f\"{current_lga}.png\"          # -- Define image file name for current LGA\n",
    "            report_sheet_name = f\"{current_lga}\"              # -- Define Excel sheet name for current LGA\n",
    "\n",
    "            # -- Step 6.6: Export results\n",
    "            export_df_to_doc_image_excel(                     # -- Export LGA-specific DataFrame to image and Excel\n",
    "                report_name=report_name,                      # -- Pass report name\n",
    "                df_shape=lga_filtered_gap,                    # -- Pass filtered LGA DataFrame for shape\n",
    "                df_style=lga_filtered_style,                  # -- Pass styled LGA DataFrame\n",
    "                img_file_name=report_image_name,              # -- Pass image file name\n",
    "                img_file_path=sub_folder2_image_file_report_rate,  # -- Pass image file path\n",
    "                doc_description=None,                         # -- No document description (not used)\n",
    "                doc_indicators_to_italicize=None,             # -- No indicators to italicize (not used)\n",
    "                doc_indicators_to_underline=None,             # -- No indicators to underline (not used)\n",
    "                xlm_file_path=doc_file_report_rate_xlsx,      # -- Pass Excel file path\n",
    "                xlm_sheet_name=report_sheet_name              # -- Pass Excel sheet name\n",
    "            )\n",
    "\n",
    "            # -- Step 6.7: Optionally display styled DataFrame\n",
    "            if display_output:                                # -- Check if display is requested\n",
    "                display(lga_filtered_style)                   # -- Display styled DataFrame for current LGA\n",
    "\n",
    "        # -- Step 7: Cache overall unfiltered DataFrame shape\n",
    "        process_facility_report_rate_gap.cached_shape = df_Report_Rate_Facility.shape  # -- Cache unfiltered DataFrame shape after processing\n",
    "\n",
    "    except Exception as e:                                    # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing facility report rate gaps: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_facility_report_rate_gap, 'cached_styles'):  # -- Check if cache exists\n",
    "            process_facility_report_rate_gap.cached_styles.clear()  # -- Clear cached styles dictionary\n",
    "        if hasattr(process_facility_report_rate_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "            del process_facility_report_rate_gap.cached_shape  # -- Clear cached shape\n",
    "        return                                            # -- Exit function on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ec5d3-cc22-4a74-bffd-8c472e3971cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### AGYW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf8e4f-4088-4095-a705-da039306b6ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - AGYW HTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba86dda-e3c6-4225-948d-1fac9573f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process AGYW HTS gap\n",
    "def process_AGYW_HTS_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process AGYW HTS gap for each LGA, exporting results as image, Excel, and Word files.\n",
    "    Caches the styled df and displays it on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the df for LGAs with gap. Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        AGYW_HTS_columns = [                                  # -- Define list of AGYW HTS columns in desired order\n",
    "            \"Number of AGYW reached with HIV Prevention Program - defined package of service during the reporting period (Community)\",\n",
    "            \"Number of AGYW reached with HIV Prevention Program - defined package of service during the reporting (Walk-In)\",\n",
    "            \"Number of AGYW that received an HIV test during the reporting period and know their status (Community)\",\n",
    "            \"Number of AGYW that received an HIV test during the reporting period and know their status (Walk-In)\"\n",
    "        ]\n",
    "        name = \"AGYW HTS gap\"                                 # -- Define general name\n",
    "        AGYW_HTS_gap_columns = ['AGYW HTS gap']               # -- Define gap column name\n",
    "        AGYW_HTS_msg = f\"No {name}\"                           # -- Define message for no gaps\n",
    "        report_name = f\"{name}1\"                              # -- Define report name\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_AGYW_HTS = prepare_and_convert_df(                 # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='AGYW_MSF',                        # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,                  # -- Use MSF hierarchy columns\n",
    "            data_columns=AGYW_HTS_columns                     # -- Include specified AGYW HTS columns\n",
    "        )\n",
    "        if df_AGYW_HTS is None:                               # -- Check if data preparation failed\n",
    "            return                                            # -- Exit function if no data\n",
    "\n",
    "        # -- Step 2.1: Add missing columns with default value and maintain order\n",
    "        for col in AGYW_HTS_columns:                          # -- Iterate over required columns\n",
    "            if col not in df_AGYW_HTS.columns:                # -- Check if column is missing\n",
    "                df_AGYW_HTS[col] = 0                          # -- Add missing column with default value 0\n",
    "\n",
    "        # -- Step 2.2: Reorder columns to match AGYW_HTS_columns plus hierarchy\n",
    "        all_columns = MSF_hierarchy + AGYW_HTS_columns         # -- Combine MSF_hierarchy and AGYW HTS columns in desired order\n",
    "        df_AGYW_HTS = df_AGYW_HTS.reindex(columns=all_columns)  # -- Reorder DataFrame columns to match all_columns\n",
    "\n",
    "        # -- Step 3: Check and display cached styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            if hasattr(process_AGYW_HTS_gap, 'cached_style'): # -- Check if cached styled DataFrame exists\n",
    "                cached_shape = getattr(process_AGYW_HTS_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_AGYW_HTS.shape             # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:             # -- Compare shapes\n",
    "                    cached_display_name = f\"✔️ Displaying {report_name} \"  # -- Define display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    print(cached_display_name)                # -- Print display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    display(process_AGYW_HTS_gap.cached_style)  # -- Display cached styled DataFrame\n",
    "                    return                                    # -- Exit function\n",
    "\n",
    "        # -- Step 4: Calculate derived metrics\n",
    "        # -- Step 4.1: Total AGYW reached with HIV Prevention\n",
    "        df_AGYW_HTS[\"Total AGYW reached with HIV Prevention\"] = (  # -- Calculate total reached\n",
    "            df_AGYW_HTS[AGYW_HTS_columns[0]] + df_AGYW_HTS[AGYW_HTS_columns[1]]  # -- Sum Community and Walk-In\n",
    "        )\n",
    "        # -- Step 4.2: Total AGYW received HIV test & know status\n",
    "        df_AGYW_HTS[\"Total AGYW received HIV test & know status\"] = (  # -- Calculate total tested\n",
    "            df_AGYW_HTS[AGYW_HTS_columns[2]] + df_AGYW_HTS[AGYW_HTS_columns[3]]  # -- Sum Community and Walk-In\n",
    "        )\n",
    "        # -- Step 4.3: AGYW HTS gap\n",
    "        df_AGYW_HTS[AGYW_HTS_gap_columns[0]] = np.where(      # -- Calculate gap\n",
    "            df_AGYW_HTS[\"Total AGYW received HIV test & know status\"] > df_AGYW_HTS[\"Total AGYW reached with HIV Prevention\"],  # -- Condition for gap\n",
    "            df_AGYW_HTS[\"Total AGYW received HIV test & know status\"] - df_AGYW_HTS[\"Total AGYW reached with HIV Prevention\"],  # -- Positive gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "\n",
    "        # -- Step 5: Filter and validate gaps\n",
    "        df_AGYW_HTS_gap = filter_gap_and_check_empty_df(      # -- Filter DataFrame for gaps\n",
    "            df=df_AGYW_HTS,                                   # -- Input DataFrame\n",
    "            msg=AGYW_HTS_msg,                                 # -- Message for empty result\n",
    "            opNonZero=AGYW_HTS_gap_columns,                   # -- Filter for non-zero gaps\n",
    "            opNeg=None,                                       # -- No negative filter\n",
    "            opPos=None,                                       # -- No positive filter\n",
    "            opZero=None,                                      # -- No zero filter\n",
    "            opLT100=None                                      # -- No less-than-100 filter\n",
    "        )\n",
    "        if df_AGYW_HTS_gap is None:                           # -- Check if no gaps found\n",
    "            if hasattr(process_AGYW_HTS_gap, 'cached_style'): # -- Check if cache exists\n",
    "                del process_AGYW_HTS_gap.cached_style         # -- Clear cached style\n",
    "            if hasattr(process_AGYW_HTS_gap, 'cached_shape'): # -- Check if cached shape exists\n",
    "                del process_AGYW_HTS_gap.cached_shape         # -- Clear cached shape\n",
    "            return                                            # -- Exit function\n",
    "\n",
    "        # -- Step 6: Style the DataFrame\n",
    "        df_AGYW_HTS_gap_style = (                             # -- Apply styling to filtered DataFrame\n",
    "            df_AGYW_HTS_gap.style                             # -- Start with DataFrame style object\n",
    "            .hide(axis='index')                               # -- Hide index column\n",
    "            .map(outlier_red, subset=AGYW_HTS_gap_columns)    # -- Highlight outliers in gap column\n",
    "        )\n",
    "\n",
    "        # -- Step 7: Cache styled DataFrame and shape\n",
    "        process_AGYW_HTS_gap.cached_style = df_AGYW_HTS_gap_style  # -- Cache styled DataFrame\n",
    "        process_AGYW_HTS_gap.cached_shape = df_AGYW_HTS.shape  # -- Cache unfiltered DataFrame shape\n",
    "\n",
    "        # -- Step 8: Define export variables\n",
    "        report_month = df_AGYW_HTS['ReportPeriod'].iloc[0]    # -- Extract report month from DataFrame\n",
    "        report_image_name = f\"{report_month}_{report_name}.png\"  # -- Define image file name\n",
    "        report_image_path = rf\"{sub_folder2_image_file_msf_outlier}\\{report_image_name}\"  # -- Define image file path\n",
    "        report_sheet_name = report_name                       # -- Define Excel sheet name\n",
    "\n",
    "        # -- Step 9: Create description for Word document\n",
    "        if (df_AGYW_HTS[AGYW_HTS_gap_columns[0]] != 0).any():  # -- Check if any gaps exist\n",
    "            report_description = (                                # -- Define report description\n",
    "                f\"Report Name: {AGYW_HTS_gap_columns[0]}\"\n",
    "                f\"\\n{AGYW_HTS_columns[2]}\\nplus {AGYW_HTS_columns[3]}\"\n",
    "                f\"\\nshould not be greater than\"\n",
    "                f\"\\n{AGYW_HTS_columns[0]}\\nplus {AGYW_HTS_columns[1]}\"\n",
    "            )\n",
    "\n",
    "        # -- Step 10: Export results\n",
    "        export_df_to_doc_image_excel(                         # -- Export DataFrame to multiple formats\n",
    "            report_name=report_name,                          # -- Pass report name\n",
    "            df_shape=df_AGYW_HTS,                             # -- Pass unfiltered DataFrame for shape\n",
    "            df_style=df_AGYW_HTS_gap_style,                   # -- Pass styled DataFrame\n",
    "            img_file_name=report_image_name,                  # -- Pass image file name\n",
    "            img_file_path=report_image_path,                  # -- Pass image file path\n",
    "            doc_file_path=doc_file_msf_outlier_docx,          # -- Pass Word document path\n",
    "            doc_description=report_description,               # -- Pass description\n",
    "            doc_indicators_to_italicize=AGYW_HTS_columns,     # -- Italicize AGYW HTS columns\n",
    "            doc_indicators_to_underline=AGYW_HTS_gap_columns, # -- Underline gap column\n",
    "            xlm_file_path=doc_file_msf_outlier_xlsx,          # -- Pass Excel file path\n",
    "            xlm_sheet_name=report_sheet_name                  # -- Pass Excel sheet name\n",
    "        )\n",
    "\n",
    "        # -- Step 11: Optionally display styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            display(df_AGYW_HTS_gap_style)                    # -- Display styled DataFrame\n",
    "\n",
    "    except Exception as e:                                    # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing {report_name}: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_AGYW_HTS_gap, 'cached_style'):     # -- Check if cache exists\n",
    "            del process_AGYW_HTS_gap.cached_style             # -- Clear cached style\n",
    "        if hasattr(process_AGYW_HTS_gap, 'cached_shape'):     # -- Check if cached shape exists\n",
    "            del process_AGYW_HTS_gap.cached_shape             # -- Clear cached shape\n",
    "        return                                                # -- Exit function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7675d4d-9704-44fe-ad84-c5eaaf9bcb92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - AGYW Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0836337-b8df-43b4-8e3b-0051aa35cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process AGYW Positive gap\n",
    "def process_AGYW_Positive_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process AGYW Positive gap, exporting results as image, Excel, and Word files.\n",
    "    Caches the styled df and displays it on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the df for gap. Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        AGYW_Positive_columns = [                             # -- Define list of AGYW Positive columns in desired order\n",
    "            \"Number of AGYW that received an HIV test during the reporting period and know their status (Community)\",\n",
    "            \"Number of AGYW that received an HIV test during the reporting period and know their status (Walk-In)\",\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Community)\",\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Walk-In)\"\n",
    "        ]\n",
    "        name = \"AGYW Positive gap\"                            # -- Define general name\n",
    "        AGYW_Positive_gap_columns = [\"AGYW tested Positive gap\"]  # -- Define gap column name\n",
    "        AGYW_Positive_msg = f\"No {name}\"                      # -- Define message for no gaps\n",
    "        report_name = f\"{name}2\"                              # -- Define report name\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_AGYW_Positive = prepare_and_convert_df(            # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='AGYW_MSF',                        # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,                  # -- Use MSF hierarchy columns\n",
    "            data_columns=AGYW_Positive_columns                # -- Include specified AGYW Positive columns\n",
    "        )\n",
    "        if df_AGYW_Positive is None:                          # -- Check if data preparation failed\n",
    "            return                                            # -- Exit function if no data\n",
    "\n",
    "        # -- Step 2.1: Add missing columns with default value\n",
    "        for col in AGYW_Positive_columns:                     # -- Iterate over required columns\n",
    "            if col not in df_AGYW_Positive.columns:           # -- Check if column is missing\n",
    "                df_AGYW_Positive[col] = 0                     # -- Add missing column with default value 0\n",
    "\n",
    "        # -- Step 2.2: Reorder columns to match AGYW_Positive_columns plus hierarchy\n",
    "        all_columns = MSF_hierarchy + AGYW_Positive_columns   # -- Combine MSF_hierarchy and AGYW Positive columns in desired order\n",
    "        df_AGYW_Positive = df_AGYW_Positive.reindex(columns=all_columns)  # -- Reorder DataFrame columns to match all_columns\n",
    "\n",
    "        # -- Step 3: Check and display cached styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            if hasattr(process_AGYW_Positive_gap, 'cached_style'):  # -- Check if cached styled DataFrame exists\n",
    "                cached_shape = getattr(process_AGYW_Positive_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_AGYW_Positive.shape        # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:             # -- Compare shapes\n",
    "                    cached_display_name = f\"✔️ Displaying {report_name} \"  # -- Define display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    print(cached_display_name)                # -- Print display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    display(process_AGYW_Positive_gap.cached_style)  # -- Display cached styled DataFrame\n",
    "                    return                                    # -- Exit function\n",
    "\n",
    "        # -- Step 4: Calculate derived metrics\n",
    "        # -- Step 4.1: Total AGYW tested\n",
    "        df_AGYW_Positive[\"Total AGYW tested\"] = (             # -- Calculate total tested\n",
    "            df_AGYW_Positive[AGYW_Positive_columns[0]] +      # -- Add Community tested\n",
    "            df_AGYW_Positive[AGYW_Positive_columns[1]]        # -- Add Walk-In tested\n",
    "        )\n",
    "        # -- Step 4.2: Total AGYW tested positive\n",
    "        df_AGYW_Positive[\"Total AGYW tested positive\"] = (    # -- Calculate total tested positive\n",
    "            df_AGYW_Positive[AGYW_Positive_columns[2]] +      # -- Add Community positive\n",
    "            df_AGYW_Positive[AGYW_Positive_columns[3]]        # -- Add Walk-In positive\n",
    "        )\n",
    "        # -- Step 4.3: AGYW tested Positive gap\n",
    "        df_AGYW_Positive[AGYW_Positive_gap_columns[0]] = np.where(  # -- Calculate gap\n",
    "            df_AGYW_Positive[\"Total AGYW tested positive\"] > df_AGYW_Positive[\"Total AGYW tested\"],  # -- Condition for gap\n",
    "            df_AGYW_Positive[\"Total AGYW tested positive\"] - df_AGYW_Positive[\"Total AGYW tested\"],  # -- Positive gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "\n",
    "        # -- Step 5: Filter and validate gaps\n",
    "        df_AGYW_Positive_gap = filter_gap_and_check_empty_df(  # -- Filter DataFrame for gaps\n",
    "            df=df_AGYW_Positive,                              # -- Input DataFrame\n",
    "            msg=AGYW_Positive_msg,                            # -- Message for empty result\n",
    "            opNonZero=AGYW_Positive_gap_columns,              # -- Filter for non-zero gaps\n",
    "            opNeg=None,                                       # -- No negative filter\n",
    "            opPos=None,                                       # -- No positive filter\n",
    "            opZero=None,                                      # -- No zero filter\n",
    "            opLT100=None                                      # -- No less-than-100 filter\n",
    "        )\n",
    "        if df_AGYW_Positive_gap is None:                      # -- Check if no gaps found\n",
    "            if hasattr(process_AGYW_Positive_gap, 'cached_style'):  # -- Check if cache exists\n",
    "                del process_AGYW_Positive_gap.cached_style    # -- Clear cached style\n",
    "            if hasattr(process_AGYW_Positive_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "                del process_AGYW_Positive_gap.cached_shape    # -- Clear cached shape\n",
    "            return                                            # -- Exit function\n",
    "\n",
    "        # -- Step 6: Style the DataFrame\n",
    "        df_AGYW_Positive_gap_style = (                        # -- Apply styling to filtered DataFrame\n",
    "            df_AGYW_Positive_gap.style                        # -- Start with DataFrame style object\n",
    "            .hide(axis='index')                               # -- Hide index column\n",
    "            .map(outlier_red, subset=AGYW_Positive_gap_columns)  # -- Highlight outliers in gap column\n",
    "        )\n",
    "\n",
    "        # -- Step 7: Cache styled DataFrame and shape\n",
    "        process_AGYW_Positive_gap.cached_style = df_AGYW_Positive_gap_style  # -- Cache styled DataFrame\n",
    "        process_AGYW_Positive_gap.cached_shape = df_AGYW_Positive.shape  # -- Cache unfiltered DataFrame shape\n",
    "\n",
    "        # -- Step 8: Define export variables\n",
    "        report_month = df_AGYW_Positive_gap['ReportPeriod'].iloc[0]  # -- Extract report month from filtered DataFrame\n",
    "        report_image_name = f\"{report_month}_{report_name}.png\"  # -- Define image file name\n",
    "        report_image_path = rf\"{sub_folder2_image_file_msf_outlier}\\{report_image_name}\"  # -- Define image file path\n",
    "        report_sheet_name = report_name                       # -- Define Excel sheet name\n",
    "\n",
    "        # -- Step 9: Create description for Word document\n",
    "        if (df_AGYW_Positive_gap[AGYW_Positive_gap_columns[0]] != 0).any():  # -- Check if any gaps exist\n",
    "            report_description = (                            # -- Define report description if gaps present\n",
    "                f\"Report Name: {AGYW_Positive_gap_columns[0]}\"\n",
    "                f\"\\n{AGYW_Positive_columns[2]}\\nplus {AGYW_Positive_columns[3]}\"\n",
    "                f\"\\nshould not be greater than\"\n",
    "                f\"\\n{AGYW_Positive_columns[0]}\\nplus {AGYW_Positive_columns[1]}\"\n",
    "            )\n",
    "\n",
    "        # -- Step 10: Export results\n",
    "        export_df_to_doc_image_excel(                         # -- Export DataFrame to multiple formats\n",
    "            report_name=report_name,                          # -- Pass report name\n",
    "            df_shape=df_AGYW_Positive_gap,                    # -- Pass filtered DataFrame for shape\n",
    "            df_style=df_AGYW_Positive_gap_style,              # -- Pass styled DataFrame\n",
    "            img_file_name=report_image_name,                  # -- Pass image file name\n",
    "            img_file_path=report_image_path,                  # -- Pass image file path\n",
    "            doc_file_path=doc_file_msf_outlier_docx,          # -- Pass Word document path\n",
    "            doc_description=report_description,               # -- Pass description\n",
    "            doc_indicators_to_italicize=AGYW_Positive_columns,  # -- Italicize AGYW Positive columns\n",
    "            doc_indicators_to_underline=AGYW_Positive_gap_columns,  # -- Underline gap column\n",
    "            xlm_file_path=doc_file_msf_outlier_xlsx,          # -- Pass Excel file path\n",
    "            xlm_sheet_name=report_sheet_name                  # -- Pass Excel sheet name\n",
    "        )\n",
    "\n",
    "        # -- Step 11: Optionally display styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            display(df_AGYW_Positive_gap_style)               # -- Display styled DataFrame\n",
    "\n",
    "    except Exception as e:                                    # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing {report_name}: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_AGYW_Positive_gap, 'cached_style'):  # -- Check if cache exists\n",
    "            del process_AGYW_Positive_gap.cached_style        # -- Clear cached style\n",
    "        if hasattr(process_AGYW_Positive_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "            del process_AGYW_Positive_gap.cached_shape        # -- Clear cached shape\n",
    "        return                                            # -- Exit function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f5aa2-9b2f-4bf2-aa58-a31cbdee5e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - AGYW Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719e7182-a5b1-4ac8-b24b-7e4f5169faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process AGYW Positive Linkage gap\n",
    "def process_AGYW_Positive_Linkage_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process AGYW Positive Linkage gap, exporting results as image, Excel, and Word files.\n",
    "    Caches the styled df and displays it on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the df for gap. Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        AGYW_Positive_Linkage_columns = [                     # -- Define list of AGYW Positive Linkage columns in desired order\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Community)\",\n",
    "            \"Number of AGYW who tested HIV Positive during the reporting period (Walk-In)\",\n",
    "            \"Total number of AGYW who tested HIV Positive and are successfully linked to treatment during the reporting period (Community & Walk-In)\",\n",
    "            \"Linked/Referred for treatment to GF supported site (subset of 4)\",\n",
    "            \"Linked/Referred for treatment to non-GF supported site (subset of 4)\",\n",
    "            \"Number of AGYW newly started on ART during the reporting period\"\n",
    "        ] \n",
    "        name = \"AGYW Positive Linkage gap\"                    # -- Define general name\n",
    "        AGYW_Positive_Linkage_gap_columns = [                 # -- Define list of gap column names\n",
    "            \"AGYW positive linked to treatment gap\",\n",
    "            \"AGYW positive linkage to GF/non-GF supported site gap\",\n",
    "            \"AGYW newly started on ART gap\"\n",
    "        ]\n",
    "        AGYW_Positive_Linkage_msg = f\"No {name}\"              # -- Define message for no gaps\n",
    "        report_name = f\"{name}3\"                              # -- Define report name\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_AGYW_Positive_Linkage = prepare_and_convert_df(    # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='AGYW_MSF',                        # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,                  # -- Use MSF hierarchy columns\n",
    "            data_columns=AGYW_Positive_Linkage_columns        # -- Include specified AGYW Positive Linkage columns\n",
    "        )\n",
    "        if df_AGYW_Positive_Linkage is None:                  # -- Check if data preparation failed\n",
    "            return                                            # -- Exit function if no data\n",
    "\n",
    "        # -- Step 2.1: Add missing columns with default value\n",
    "        for col in AGYW_Positive_Linkage_columns:             # -- Iterate over required columns\n",
    "            if col not in df_AGYW_Positive_Linkage.columns:   # -- Check if column is missing\n",
    "                df_AGYW_Positive_Linkage[col] = 0             # -- Add missing column with default value 0\n",
    "\n",
    "        # -- Step 2.2: Reorder columns to match AGYW_Positive_Linkage_columns plus hierarchy\n",
    "        all_columns = MSF_hierarchy + AGYW_Positive_Linkage_columns  # -- Combine hierarchy and AGYW Positive Linkage columns in desired order\n",
    "        df_AGYW_Positive_Linkage = df_AGYW_Positive_Linkage.reindex(columns=all_columns)  # -- Reorder DataFrame columns to match all_columns\n",
    "\n",
    "        # -- Step 3: Check and display cached styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            if hasattr(process_AGYW_Positive_Linkage_gap, 'cached_style'):  # -- Check if cached styled DataFrame exists\n",
    "                cached_shape = getattr(process_AGYW_Positive_Linkage_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_AGYW_Positive_Linkage.shape  # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:             # -- Compare shapes\n",
    "                    cached_display_name = f\"✔️ Displaying {report_name} \"  # -- Define display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    print(cached_display_name)                # -- Print display message\n",
    "                    print(f\"-\" * len(cached_display_name))    # -- Print separator line\n",
    "                    display(process_AGYW_Positive_Linkage_gap.cached_style)  # -- Display cached styled DataFrame\n",
    "                    return                                    # -- Exit function\n",
    "\n",
    "        # -- Step 4: Calculate derived metrics\n",
    "        # -- Step 4.1: Total AGYW Tested Positive\n",
    "        df_AGYW_Positive_Linkage[\"Total AGYW Tested Positive\"] = (  # -- Calculate total tested positive\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[0]] +  # -- Add Community positive\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[1]]    # -- Add Walk-In positive\n",
    "        )\n",
    "        # -- Step 4.2: AGYW positive linked to treatment gap\n",
    "        df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_gap_columns[0]] = np.where(  # -- Calculate linkage gap\n",
    "            df_AGYW_Positive_Linkage[\"Total AGYW Tested Positive\"] != df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]],  # -- Condition for gap\n",
    "            df_AGYW_Positive_Linkage[\"Total AGYW Tested Positive\"] - df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]],  # -- Gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "        # -- Step 4.3: AGYW positive linkage to GF/non-GF supported site gap\n",
    "        total_linked_to_sites = (                             # -- Precompute total linked to sites\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[3]] +  # -- Add GF supported site\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[4]]    # -- Add non-GF supported site\n",
    "        )\n",
    "        df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_gap_columns[1]] = np.where(  # -- Calculate site linkage gap\n",
    "            total_linked_to_sites != df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]],  # -- Condition for gap\n",
    "            total_linked_to_sites - df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]],  # -- Gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "        # -- Step 4.4: AGYW newly started on ART gap\n",
    "        df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_gap_columns[2]] = np.where(  # -- Calculate ART gap\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]] != df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[5]],  # -- Condition for gap\n",
    "            df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[2]] - df_AGYW_Positive_Linkage[AGYW_Positive_Linkage_columns[5]],  # -- Gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "\n",
    "        # -- Step 5: Filter and validate gaps\n",
    "        df_AGYW_Positive_Linkage_gap = filter_gap_and_check_empty_df(  # -- Filter DataFrame for gaps\n",
    "            df=df_AGYW_Positive_Linkage,                      # -- Input DataFrame\n",
    "            msg=AGYW_Positive_Linkage_msg,                    # -- Message for empty result\n",
    "            opNonZero=AGYW_Positive_Linkage_gap_columns,      # -- Filter for non-zero gaps\n",
    "            opNeg=None,                                       # -- No negative filter\n",
    "            opPos=None,                                       # -- No positive filter\n",
    "            opZero=None,                                      # -- No zero filter\n",
    "            opLT100=None                                      # -- No less-than-100 filter\n",
    "        )\n",
    "        if df_AGYW_Positive_Linkage_gap is None:              # -- Check if no gaps found\n",
    "            if hasattr(process_AGYW_Positive_Linkage_gap, 'cached_style'):  # -- Check if cache exists\n",
    "                del process_AGYW_Positive_Linkage_gap.cached_style  # -- Clear cached style\n",
    "            if hasattr(process_AGYW_Positive_Linkage_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "                del process_AGYW_Positive_Linkage_gap.cached_shape  # -- Clear cached shape\n",
    "            return                                            # -- Exit function\n",
    "\n",
    "        # -- Step 6: Style the DataFrame\n",
    "        df_AGYW_Positive_Linkage_gap_style = (                # -- Apply styling to filtered DataFrame\n",
    "            df_AGYW_Positive_Linkage_gap.style                # -- Start with DataFrame style object\n",
    "            .hide(axis='index')                               # -- Hide index column\n",
    "            .map(outlier_red, subset=AGYW_Positive_Linkage_gap_columns)  # -- Highlight outliers in gap columns\n",
    "        )\n",
    "\n",
    "        # -- Step 7: Cache styled DataFrame and shape\n",
    "        process_AGYW_Positive_Linkage_gap.cached_style = df_AGYW_Positive_Linkage_gap_style  # -- Cache styled DataFrame\n",
    "        process_AGYW_Positive_Linkage_gap.cached_shape = df_AGYW_Positive_Linkage.shape  # -- Cache unfiltered DataFrame shape\n",
    "\n",
    "        # -- Step 8: Define export variables\n",
    "        report_month = df_AGYW_Positive_Linkage_gap['ReportPeriod'].iloc[0]  # -- Extract report month from filtered DataFrame\n",
    "        report_image_name = f\"{report_month}_{report_name}.png\"  # -- Define image file name\n",
    "        report_image_path = rf\"{sub_folder2_image_file_msf_outlier}\\{report_image_name}\"  # -- Define image file path\n",
    "        report_sheet_name = report_name                       # -- Define Excel sheet name\n",
    "\n",
    "        # -- Step 9: Create descriptions for Word document\n",
    "        report_description = []                               # -- Initialize list to collect descriptions\n",
    "        # -- Step 9.1: Add description for AGYW Positive Linkage gap\n",
    "        if (df_AGYW_Positive_Linkage_gap[AGYW_Positive_Linkage_gap_columns[0]] != 0).any():  # -- Check if linkage gap exists\n",
    "            report_description += (                           # -- Add description for linkage gap\n",
    "                f\"Report Name: {AGYW_Positive_Linkage_gap_columns[0]}\"\n",
    "                f\"\\n{AGYW_Positive_Linkage_columns[0]}\\nplus {AGYW_Positive_Linkage_columns[1]}\"\n",
    "                f\"\\nshould be equal to {AGYW_Positive_Linkage_columns[2]}\"\n",
    "                f\"\\nNote: Where this AGYW linkage gap is true, please ignore the outlier.\"\n",
    "            )\n",
    "        # -- Step 9.2: Add description for AGYW Positive Linkage to GF/non-GF supported site gap\n",
    "        if (df_AGYW_Positive_Linkage_gap[AGYW_Positive_Linkage_gap_columns[1]] != 0).any():  # -- Check if site linkage gap exists\n",
    "            report_description.append(                        # -- Add description for site linkage gap\n",
    "                f\"Report Name: {AGYW_Positive_Linkage_gap_columns[1]}\"\n",
    "                f\"\\n{AGYW_Positive_Linkage_columns[3]}\\nplus {AGYW_Positive_Linkage_columns[4]}\"\n",
    "                f\"\\nshould be equal to {AGYW_Positive_Linkage_columns[2]}\"\n",
    "            )\n",
    "        # -- Step 9.3: Add description for AGYW newly started on ART gap\n",
    "        if (df_AGYW_Positive_Linkage_gap[AGYW_Positive_Linkage_gap_columns[2]] != 0).any():  # -- Check if ART gap exists\n",
    "            report_description.append(                        # -- Add description for ART gap\n",
    "                f\"Report Name: {AGYW_Positive_linkage_gap_columns[2]}\"\n",
    "                f\"\\n{AGYW_Positive_Linkage_columns[5]}\"\n",
    "                f\"\\nshould be equal to {AGYW_Positive_Linkage_columns[2]}\"\n",
    "            )\n",
    "        # -- Step 9.4: Join all descriptions\n",
    "        report_description = \"\\n\".join(report_description)\n",
    "\n",
    "        # -- Step 10: Export results\n",
    "        export_df_to_doc_image_excel(                         # -- Export DataFrame to multiple formats\n",
    "            report_name=report_name,                          # -- Pass report name\n",
    "            df_shape=df_AGYW_Positive_Linkage_gap,            # -- Pass filtered DataFrame for shape\n",
    "            df_style=df_AGYW_Positive_Linkage_gap_style,      # -- Pass styled DataFrame\n",
    "            img_file_name=report_image_name,                  # -- Pass image file name\n",
    "            img_file_path=report_image_path,                  # -- Pass image file path\n",
    "            doc_file_path=doc_file_msf_outlier_docx,          # -- Pass Word document path\n",
    "            doc_description=report_description,               # -- Pass description\n",
    "            doc_indicators_to_italicize=AGYW_Positive_Linkage_columns,  # -- Italicize AGYW Positive Linkage columns\n",
    "            doc_indicators_to_underline=AGYW_Positive_Linkage_gap_columns,  # -- Underline gap columns\n",
    "            xlm_file_path=doc_file_msf_outlier_xlsx,          # -- Pass Excel file path\n",
    "            xlm_sheet_name=report_sheet_name                  # -- Pass Excel sheet name\n",
    "        )\n",
    "\n",
    "        # -- Step 11: Optionally display styled DataFrame\n",
    "        if display_output:                                    # -- Check if display is requested\n",
    "            display(df_AGYW_Positive_Linkage_gap_style)       # -- Display styled DataFrame\n",
    "\n",
    "    except Exception as e:                                    # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing {report_name}: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_AGYW_Positive_Linkage_gap, 'cached_style'):  # -- Check if cache exists\n",
    "            del process_AGYW_Positive_Linkage_gap.cached_style  # -- Clear cached style\n",
    "        if hasattr(process_AGYW_Positive_Linkage_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "            del process_AGYW_Positive_Linkage_gap.cached_shape  # -- Clear cached shape\n",
    "        return                                            # -- Exit function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c60e1d-1d65-4bf5-b353-3dad03df1d94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - AGYW TB Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ceb37e-cf14-4ee8-a62b-422e0d88668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process AGYW TB Screening gap\n",
    "def process_AGYW_TB_Screening_gap(display_output=None):\n",
    "    \"\"\"\n",
    "    Process AGYW TB Screening gap, exporting results as image, Excel, and Word files.\n",
    "    Caches the styled df and displays it on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the df for gap. Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        AGYW_TB_Screening_columns = [                     # -- Define list of AGYW TB Screening columns in desired order\n",
    "            \"Number of AGYW newly started on ART during the reporting period\",\n",
    "            \"Number of AGYW screened for TB amongst those newly started on ART during the reporting period\"\n",
    "        ]\n",
    "        name = \"AGYW TB Screening gap\"                    # -- Define general name\n",
    "        AGYW_TB_Screening_gap_columns = [\"AGYW TB screening gap\"]  # -- Define list of gap column names\n",
    "        AGYW_TB_Screening_msg = f\"No {name}\"              # -- Define message for no gaps\n",
    "        report_name = f\"{name}4\"                          # -- Define report name\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_AGYW_TB_Screening = prepare_and_convert_df(    # -- Fetch and prepare DataFrame from DHIS2 data\n",
    "            DHIS2_data_key='AGYW_MSF',                    # -- Specify DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,              # -- Use MSF hierarchy columns\n",
    "            data_columns=AGYW_TB_Screening_columns        # -- Include specified AGYW TB Screening columns\n",
    "        )\n",
    "        if df_AGYW_TB_Screening is None:                  # -- Check if data preparation failed\n",
    "            return                                        # -- Exit function if no data\n",
    "\n",
    "        # -- Step 2.1: Add missing columns with default value\n",
    "        for col in AGYW_TB_Screening_columns:             # -- Iterate over required columns\n",
    "            if col not in df_AGYW_TB_Screening.columns:   # -- Check if column is missing\n",
    "                df_AGYW_TB_Screening[col] = 0             # -- Add missing column with default value 0\n",
    "\n",
    "        # -- Step 2.2: Reorder columns to match AGYW_TB_Screening_columns plus hierarchy\n",
    "        all_columns = MSF_hierarchy + AGYW_TB_Screening_columns  # -- Combine MSF_hierarchy and AGYW TB Screening columns in desired order\n",
    "        df_AGYW_TB_Screening = df_AGYW_TB_Screening.reindex(columns=all_columns)  # -- Reorder DataFrame columns to match all_columns\n",
    "\n",
    "        # -- Step 3: Check and display cached styled DataFrame\n",
    "        if display_output:                                 # -- Check if display is requested\n",
    "            if hasattr(process_AGYW_TB_Screening_gap, 'cached_style'):  # -- Check if cached styled DataFrame exists\n",
    "                cached_shape = getattr(process_AGYW_TB_Screening_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_AGYW_TB_Screening.shape  # -- Get current unfiltered shape\n",
    "                if cached_shape == current_shape:           # -- Compare shapes\n",
    "                    cached_display_name = f\"✔️ Displaying {report_name} \"  # -- Define display message\n",
    "                    print(f\"-\" * len(cached_display_name))  # -- Print separator line\n",
    "                    print(cached_display_name)              # -- Print display message\n",
    "                    print(f\"-\" * len(cached_display_name))  # -- Print separator line\n",
    "                    display(process_AGYW_TB_Screening_gap.cached_style)  # -- Display cached styled DataFrame\n",
    "                    return                                  # -- Exit function\n",
    "\n",
    "        # -- Step 4: Calculate derived metrics\n",
    "        # -- Step 4.1: AGYW TB Screening gap\n",
    "        df_AGYW_TB_Screening[AGYW_TB_Screening_gap_columns[0]] = np.where(  # -- Calculate TB Screening gap\n",
    "            df_AGYW_TB_Screening[AGYW_TB_Screening_columns[1]] != df_AGYW_TB_Screening[AGYW_TB_Screening_columns[0]],  # -- Condition for gap\n",
    "            df_AGYW_TB_Screening[AGYW_TB_Screening_columns[1]] - df_AGYW_TB_Screening[AGYW_TB_Screening_columns[0]],  # -- Gap value\n",
    "            0                                                 # -- Default to 0 if no gap\n",
    "        )\n",
    "\n",
    "        # -- Step 5: Filter and validate gaps\n",
    "        df_AGYW_TB_Screening_gap = filter_gap_and_check_empty_df(  # -- Filter DataFrame for gaps\n",
    "            df=df_AGYW_TB_Screening,                      # -- Input DataFrame\n",
    "            msg=AGYW_TB_Screening_msg,                    # -- Message for empty result\n",
    "            opNonZero=AGYW_TB_Screening_gap_columns,      # -- Filter for non-zero gaps\n",
    "            opNeg=None,                                   # -- No negative filter\n",
    "            opPos=None,                                   # -- No positive filter\n",
    "            opZero=None,                                  # -- No zero filter\n",
    "            opLT100=None                                  # -- No less-than-100 filter\n",
    "        )\n",
    "        if df_AGYW_TB_Screening_gap is None:              # -- Check if no gaps found\n",
    "            if hasattr(process_AGYW_TB_Screening_gap, 'cached_style'):  # -- Check if cache exists\n",
    "                del process_AGYW_TB_Screening_gap.cached_style  # -- Clear cached style\n",
    "            if hasattr(process_AGYW_TB_Screening_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "                del process_AGYW_TB_Screening_gap.cached_shape  # -- Clear cached shape\n",
    "            return                                        # -- Exit function\n",
    "\n",
    "        # -- Step 6: Style the DataFrame\n",
    "        df_AGYW_TB_Screening_gap_style = (                # -- Apply styling to filtered DataFrame\n",
    "            df_AGYW_TB_Screening_gap.style                # -- Start with DataFrame style object\n",
    "            .hide(axis='index')                           # -- Hide index column\n",
    "            .map(outlier_red, subset=AGYW_TB_Screening_gap_columns)  # -- Highlight outliers in gap columns\n",
    "        )\n",
    "\n",
    "        # -- Step 7: Cache styled DataFrame and shape\n",
    "        process_AGYW_TB_Screening_gap.cached_style = df_AGYW_TB_Screening_gap_style  # -- Cache styled DataFrame\n",
    "        process_AGYW_TB_Screening_gap.cached_shape = df_AGYW_TB_Screening.shape  # -- Cache unfiltered DataFrame shape\n",
    "\n",
    "        # -- Step 8: Define export variables\n",
    "        report_month = df_AGYW_TB_Screening_gap['ReportPeriod'].iloc[0]  # -- Extract report month from filtered DataFrame\n",
    "        report_image_name = f\"{report_month}_{report_name}.png\"  # -- Define image file name\n",
    "        report_image_path = rf\"{sub_folder2_image_file_msf_outlier}\\{report_image_name}\"  # -- Define image file path\n",
    "        report_sheet_name = report_name                   # -- Define Excel sheet name\n",
    "\n",
    "        # -- Step 9: Create descriptions for Word document\n",
    "        # -- Step 9.1: Add description for AGYW Positive Linkage gap\n",
    "        if (df_AGYW_TB_Screening_gap[AGYW_TB_Screening_gap_columns[0]] != 0).any():  # -- Check if TB Screening gap exists\n",
    "            report_description = (                        # -- Add description for TB Screening gap\n",
    "                f\"Report Name: {AGYW_TB_Screening_gap_columns[0]}\"\n",
    "                f\"\\n{AGYW_TB_Screening_columns[1]}\\nshould be equal to {AGYW_TB_Screening_columns[0]}\"\n",
    "            )\n",
    "\n",
    "        # -- Step 10: Export results\n",
    "        export_df_to_doc_image_excel(                     # -- Export DataFrame to multiple formats\n",
    "            report_name=report_name,                      # -- Pass report name\n",
    "            df_shape=df_AGYW_TB_Screening_gap,            # -- Pass filtered DataFrame for shape\n",
    "            df_style=df_AGYW_TB_Screening_gap_style,      # -- Pass styled DataFrame\n",
    "            img_file_name=report_image_name,              # -- Pass image file name\n",
    "            img_file_path=report_image_path,              # -- Pass image file path\n",
    "            doc_file_path=doc_file_msf_outlier_docx,      # -- Pass Word document path\n",
    "            doc_description=report_description,           # -- Pass description\n",
    "            doc_indicators_to_italicize=AGYW_TB_Screening_columns,  # -- Italicize AGYW Positive Linkage columns\n",
    "            doc_indicators_to_underline=AGYW_TB_Screening_gap_columns,  # -- Underline gap columns\n",
    "            xlm_file_path=doc_file_msf_outlier_xlsx,      # -- Pass Excel file path\n",
    "            xlm_sheet_name=report_sheet_name              # -- Pass Excel sheet name\n",
    "        )\n",
    "\n",
    "        # -- Step 11: Optionally display styled DataFrame\n",
    "        if display_output:                                # -- Check if display is requested\n",
    "            display(df_AGYW_TB_Screening_gap_style)       # -- Display styled DataFrame\n",
    "\n",
    "    except Exception as e:                                # -- Catch any exceptions\n",
    "        print(f\"⦸ Error processing {report_name}: {str(e)}\")  # -- Print error message\n",
    "        if hasattr(process_AGYW_TB_Screening_gap, 'cached_style'):  # -- Check if cache exists\n",
    "            del process_AGYW_TB_Screening_gap.cached_style  # -- Clear cached style\n",
    "        if hasattr(process_AGYW_TB_Screening_gap, 'cached_shape'):  # -- Check if cached shape exists\n",
    "            del process_AGYW_TB_Screening_gap.cached_shape  # -- Clear cached shape\n",
    "        return                                            # -- Exit function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503ceb-91b9-4a14-a53f-371916158c75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98450f84-6881-4b7a-971d-f09a53acf359",
   "metadata": {},
   "source": [
    "#### - ART Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac2e50d-78e2-4f50-bbd8-087bebb4eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Define the main function to process ART positive and enrollment gap\n",
    "def process_ART_PosEnrolment_gap(display_output=None):          # -- Define function\n",
    "    \"\"\"\n",
    "    Process ART Positive and Enrolment gap, exporting results as image, Excel, and Word files.\n",
    "    Caches the styled df and displays it on subsequent calls if data shape unchanged.\n",
    "    \n",
    "    Args:\n",
    "        display_output (bool, optional): If True, displays the df for gap. Defaults to None (treated as False unless explicitly True).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # -- Step 1: Initialize constants\n",
    "        ART_PosEnrolment_columns = [                       # -- Define list of ART positive and enrollment columns\n",
    "            \"ART 1: Number of HIV positive persons newly enrolled in clinical care during the month\",\n",
    "            \"ART 2: Number of people living with HIV newly started on ART during the month (excludes ART transfer-in)\"\n",
    "        ]\n",
    "        ART_PosEnrolment_columns_desc = ART_PosEnrolment_columns + [\"Total Tested Positive\"]  # -- Extended list for description\n",
    "        name = \"ART Positive-Enrolment gap\"                # -- General report name\n",
    "        ART_PosEnrolment_gap_columns = [\"ART Enrolment gap\", \"ART Linkage gap\"]  # -- Gap column names\n",
    "        report_name = f\"{name}5\"                           # -- Report name with suffix\n",
    "\n",
    "        # -- Step 2: Prepare data\n",
    "        df_ART_PosEnrolment = prepare_and_convert_df(      # -- Fetch and prepare DataFrame\n",
    "            DHIS2_data_key='ART_MSF',                      # -- DHIS2 data key\n",
    "            hierarchy_columns=MSF_hierarchy,               # -- MSF hierarchy columns\n",
    "            data_columns=ART_PosEnrolment_columns          # -- ART columns\n",
    "        )\n",
    "\n",
    "        # -- Step 3: Merge with external DataFrame\n",
    "        df_ART_PosEnrolment = Pre_MSF_positives_all.merge(  # -- Merge with positives data\n",
    "            df_ART_PosEnrolment,                           # -- Merge target\n",
    "            on=[\"ReportPeriod\", \"Cluster\", \"LGA\", \"FacilityName\"],  # -- Merge keys\n",
    "            how=\"right\"                                    # -- Keep all rows from df_ART_PosEnrolment\n",
    "        )\n",
    "        df_ART_PosEnrolment = df_ART_PosEnrolment.fillna(0)  # -- Step 3.1: Fill NaN with 0\n",
    "        float_columns = df_ART_PosEnrolment.select_dtypes(include=['float64', 'float32']).columns  # -- Identify float columns\n",
    "        for col in float_columns:                          # -- Step 3.2: Cast float to int\n",
    "            df_ART_PosEnrolment[col] = df_ART_PosEnrolment[col].astype(int)\n",
    "\n",
    "        # -- Step 6: Calculate derived metrics\n",
    "        df_ART_PosEnrolment[ART_PosEnrolment_gap_columns[0]] = np.where(  # -- Step 6.1: ART enrolment gap\n",
    "            df_ART_PosEnrolment[ART_PosEnrolment_columns[0]] != df_ART_PosEnrolment[\"Total new positive\"],\n",
    "            df_ART_PosEnrolment[ART_PosEnrolment_columns[0]] - df_ART_PosEnrolment[\"Total new positive\"],\n",
    "            0\n",
    "        )\n",
    "        df_ART_PosEnrolment[ART_PosEnrolment_gap_columns[1]] = np.where(  # -- Step 6.2: ART linkage gap\n",
    "            df_ART_PosEnrolment[ART_PosEnrolment_columns[1]] != df_ART_PosEnrolment[ART_PosEnrolment_columns[0]],\n",
    "            df_ART_PosEnrolment[ART_PosEnrolment_columns[1]] - df_ART_PosEnrolment[ART_PosEnrolment_columns[0]],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # -- Step 4: Check and display cached styled DataFrames\n",
    "        if display_output:                                 # -- Check if display requested\n",
    "            if hasattr(process_ART_PosEnrolment_gap, 'cached_styles'):  # -- Check cache existence\n",
    "                cached_shape = getattr(process_ART_PosEnrolment_gap, 'cached_shape', None)  # -- Get cached shape\n",
    "                current_shape = df_ART_PosEnrolment.shape  # -- Get current shape\n",
    "                if cached_shape == current_shape:          # -- Compare shapes\n",
    "                    print(\"Using cached styles\")           # -- Confirm cache hit\n",
    "                    for cluster, style in process_ART_PosEnrolment_gap.cached_styles.items():  # -- Iterate cached styles\n",
    "                        cached_display_name = f\"✔️ Displaying {cluster} {report_name} \"  # -- Display message\n",
    "                        print(\"-\" * len(cached_display_name))  # -- Separator\n",
    "                        print(cached_display_name)            # -- Message\n",
    "                        print(\"-\" * len(cached_display_name))  # -- Separator\n",
    "                        display(style)                        # -- Display cached style\n",
    "                    return                                 # -- Exit if cache used\n",
    "                    \n",
    "        # -- Step 5: Initialize cache\n",
    "        if not hasattr(process_ART_PosEnrolment_gap, 'cached_styles'):  # -- Check if cache exists\n",
    "            process_ART_PosEnrolment_gap.cached_styles = {}  # -- Initialize cache\n",
    "\n",
    "        # -- Step 7: Identify unique clusters\n",
    "        cluster_list = pd.Series(df_ART_PosEnrolment['Cluster'].unique())  # -- Extract unique clusters\n",
    "\n",
    "        # -- Step 8: Process each cluster\n",
    "        for current_cluster in cluster_list:               # -- Iterate over clusters\n",
    "            cluster_filtered = df_ART_PosEnrolment[df_ART_PosEnrolment['Cluster'] == current_cluster]  # -- Step 8.1: Filter cluster\n",
    "            \n",
    "            ART_PosEnrolment_msg = f\"No {current_cluster} {report_name}\"  # -- Message for no gaps\n",
    "\n",
    "            cluster_filtered_gap = filter_gap_and_check_empty_df(  # -- Step 8.2: Filter gaps\n",
    "                df=cluster_filtered,\n",
    "                msg=ART_PosEnrolment_msg,\n",
    "                opNonZero=ART_PosEnrolment_gap_columns,\n",
    "                opNeg=None,\n",
    "                opPos=None,\n",
    "                opZero=None,\n",
    "                opLT100=None\n",
    "            )\n",
    "\n",
    "            if cluster_filtered_gap is None:               # -- Check if no gaps\n",
    "                if current_cluster in process_ART_PosEnrolment_gap.cached_styles:  # -- Remove from cache\n",
    "                    del process_ART_PosEnrolment_gap.cached_styles[current_cluster]\n",
    "                continue                                   # -- Skip cluster\n",
    "\n",
    "            cluster_filtered_style = (                     # -- Step 8.3: Style DataFrame\n",
    "                cluster_filtered_gap.style\n",
    "                .hide(axis='index')\n",
    "                .map(outlier_red, subset=ART_PosEnrolment_gap_columns)\n",
    "            )\n",
    "\n",
    "            process_ART_PosEnrolment_gap.cached_styles[current_cluster] = cluster_filtered_style  # -- Step 8.4: Cache style\n",
    "\n",
    "            # -- Step 8.5: Define export variables\n",
    "            report_name_cluster = f\"{current_cluster}_{report_name}\"  # -- Cluster-specific report name\n",
    "            report_month = cluster_filtered_gap['ReportPeriod'].iloc[0]  # -- Extract report month\n",
    "            report_image_name = f\"{report_month}_{report_name_cluster}.png\"  # -- Image file name\n",
    "            report_image_path = rf\"{sub_folder2_image_file_msf_outlier}\\{report_image_name}\"  # -- Image path\n",
    "            report_sheet_name = f\"{current_cluster}_{report_name}\"  # -- Excel sheet name\n",
    "\n",
    "            # -- Step 9: Create descriptions\n",
    "            report_description = []                        # -- Initialize descriptions\n",
    "            if (cluster_filtered_gap[ART_PosEnrolment_gap_columns[0]] != 0).any():  # -- Step 9.1: Enrolment gap desc\n",
    "                report_description.append(\n",
    "                    f\"Report Name: {ART_PosEnrolment_gap_columns[0]}\\n\"\n",
    "                    f\"{ART_PosEnrolment_columns_desc[0]}\\nshould be equal to {ART_PosEnrolment_columns_desc[2]}\\n\"\n",
    "                    f\"Note: Where this ART enrolment gap is true, please ignore the outlier.\"\n",
    "                )\n",
    "            if (cluster_filtered_gap[ART_PosEnrolment_gap_columns[1]] != 0).any():  # -- Step 9.2: Linkage gap desc\n",
    "                report_description.append(\n",
    "                    f\"Report Name: {ART_PosEnrolment_gap_columns[1]}\\n\"\n",
    "                    f\"{ART_PosEnrolment_columns_desc[1]}\\nshould be equal to {ART_PosEnrolment_columns_desc[0]}\\n\"\n",
    "                    f\"Note: Where this ART linkage gap is true, please ignore the outlier.\"\n",
    "                )\n",
    "            report_description = \"\\n\\n\".join(report_description)  # -- Step 9.3: Join descriptions\n",
    "\n",
    "            # -- Step 10: Export results\n",
    "            export_df_to_doc_image_excel(                  # -- Export DataFrame\n",
    "                report_name=report_name_cluster,\n",
    "                df_shape=cluster_filtered_gap,\n",
    "                df_style=cluster_filtered_style,\n",
    "                img_file_name=report_image_name,\n",
    "                img_file_path=sub_folder2_image_file_msf_outlier,\n",
    "                doc_description=report_description,\n",
    "                doc_indicators_to_italicize=ART_PosEnrolment_columns_desc,\n",
    "                doc_indicators_to_underline=ART_PosEnrolment_gap_columns,\n",
    "                xlm_file_path=doc_file_msf_outlier_xlsx,\n",
    "                xlm_sheet_name=report_sheet_name\n",
    "            )\n",
    "\n",
    "            if display_output:                             # -- Step 11: Display styled DataFrame\n",
    "                display(cluster_filtered_style)\n",
    "\n",
    "        # -- Step 12: Cache overall unfiltered DataFrame shape\n",
    "        process_ART_PosEnrolment_gap.cached_shape = df_ART_PosEnrolment.shape  # -- Cache shape\n",
    "\n",
    "    except Exception as e:                                 # -- Catch exceptions\n",
    "        print(f\"⦸ Error processing {report_name}: {str(e)}\")  # -- Print error\n",
    "        if hasattr(process_ART_PosEnrolment_gap, 'cached_styles'):  # -- Clear cache on error\n",
    "            process_ART_PosEnrolment_gap.cached_styles.clear()\n",
    "        if hasattr(process_ART_PosEnrolment_gap, 'cached_shape'):  # -- Clear shape on error\n",
    "            del process_ART_PosEnrolment_gap.cached_shape\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eea766-8a87-43e4-aef8-ec79604a368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3bc25-ae6d-4ee8-bf86-039d7e0f061e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80980f5-5c31-4577-81e2-b8918fedf27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90fed5-62bf-4153-9c27-52ef53cae7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c2556-854c-465f-9e0d-bac7d46b1a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db282c-96a1-42e7-bbd1-40ecc111bc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a58abb-0bde-4c1e-a614-10e7b141c454",
   "metadata": {},
   "source": [
    "### Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018fb0e5-ba42-4cdf-83e8-ced3d4616b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981209126ab341669df69643b76fbb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='General Actions ▼', layout=Layout(width='150px'), style=Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## -- List of function descriptions for display\n",
    "function_description_name = [\n",
    "    \"Get Data\",                                # -- Description for LGA report rate gap\n",
    "    \"Generate Report\",                         # -- Description for report generating\n",
    "    \"ANSO Report Rate\",                        # -- Description for LGA report rate gap\n",
    "    \"LGA Report Rate\",                         # -- Description for facility report rate gap\n",
    "    \"AGYW HTS\",                                # -- Description for AGYW HTS gap\n",
    "    \"AGYW Pos\",                                # -- Description for AGYW positive gap\n",
    "    \"AGYW Pos Linkage\",                        # -- Description for AGYW positive linkage gap \n",
    "    \"AGYW TB Screening\",                       # -- Description for AGYW TB screening gap\n",
    "    \"ART PosEnrolment\"                         # -- Description for ART positive enrolment gap\n",
    "]\n",
    "\n",
    "# -- Define constants for UI elements\n",
    "ui_title = \"ANSO IHVN DHIS2 MSF Console\"       # -- Report title to be displayed in bold\n",
    "author = \"Reuben Edidiong\"                     # -- Author name, kept plain due to terminal italic limitation\n",
    "version = \"msf.vlr v1.0\"                       # -- Version identifier for the report\n",
    "ui_sepperator_line = 150                       # -- Length of separator lines (adjustable; 80 for cleaner look)\n",
    "bold = \"\\033[1m\"                               # -- ANSI code for bold text\n",
    "reset = \"\\033[0m\"                              # -- ANSI code to reset formatting\n",
    "\n",
    "# -- Core components for separators\n",
    "header = f\"{bold}{ui_title}{reset} {f'© {author} {version}':>122}\"  # -- Header with bold title, right-aligned copyright\n",
    "top_line = f\"{'-' * ui_sepperator_line}\\n\"       # -- Top separator line\n",
    "bottom_line = f\"\\n{'-' * ui_sepperator_line}\"  # -- Bottom separator line\n",
    "spacing = \"\\n\" * 15                            # -- Empty line gap in ui_separator_clear\n",
    "\n",
    "# -- Separator definitions\n",
    "ui_separator_top = f\"{header}\\n{top_line}\"     # -- Top separator: header followed by a single line\n",
    "ui_separator_bottom = f\"{bottom_line}\"         # -- Bottom separator: just a single line\n",
    "ui_separator_clear = (                         # -- Full clear separator: header, top line, spacing, bottom line\n",
    "    f\"{header}\\n\"\n",
    "    f\"{top_line}\"\n",
    "    f\"{spacing}\"\n",
    "    f\"{bottom_line}\"\n",
    ")\n",
    "\n",
    "def run_jupyter_mode():\n",
    "    \"\"\"\n",
    "    Runs an interactive Jupyter interface with buttons to execute report processing functions.\n",
    "    Group names are displayed horizontally with a dropdown arrow, bold text, and font size increased by 2 points.\n",
    "    Sub-buttons appear when a group name is clicked.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None \n",
    "    \"\"\"\n",
    "\n",
    "    # -- Step 1: Create buttons with descriptive labels\n",
    "    botton0 = widgets.Button(description=f\"{function_description_name[0]}\")\n",
    "    botton1 = widgets.Button(description=f\"{function_description_name[1]}\")\n",
    "    botton2 = widgets.Button(description=f\"{function_description_name[2]}\")\n",
    "    botton3 = widgets.Button(description=f\"{function_description_name[3]}\")\n",
    "    botton4 = widgets.Button(description=f\"{function_description_name[4]}\")\n",
    "    botton5 = widgets.Button(description=f\"{function_description_name[5]}\")\n",
    "    botton6 = widgets.Button(description=f\"{function_description_name[6]}\")\n",
    "    botton7 = widgets.Button(description=f\"{function_description_name[7]}\")\n",
    "    botton8 = widgets.Button(description=f\"{function_description_name[8]}\")\n",
    "    clear_button = widgets.Button(description=\"Clear screen\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # -- Step 2: Define button handlers\n",
    "    def on_botton0_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            fetch_dhis2_data_interactive_jupyter_mode()\n",
    "            print(ui_separator_bottom)\n",
    "            \n",
    "    def on_botton2_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_lga_report_rate_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton3_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_facility_report_rate_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton4_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_AGYW_HTS_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton5_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_AGYW_Positive_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton6_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_AGYW_Positive_Linkage_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton7_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_AGYW_TB_Screening_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_botton8_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_ART_PosEnrolment_gap(display_output=True)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    def on_clear_button_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_clear)\n",
    "\n",
    "    def on_botton1_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(ui_separator_top)\n",
    "            process_lga_report_rate_gap(display_output=False)\n",
    "            process_facility_report_rate_gap(display_output=False)\n",
    "            process_AGYW_HTS_gap(display_output=False)\n",
    "            process_AGYW_Positive_gap(display_output=False)\n",
    "            process_AGYW_Positive_Linkage_gap(display_output=False)\n",
    "            process_AGYW_TB_Screening_gap(display_output=False)\n",
    "            process_ART_PosEnrolment_gap(display_output=False)\n",
    "            print(ui_separator_bottom)\n",
    "\n",
    "    # -- Step 3: Link buttons to their handlers\n",
    "    botton0.on_click(on_botton0_click)\n",
    "    botton1.on_click(on_botton1_click)\n",
    "    botton2.on_click(on_botton2_click)\n",
    "    botton3.on_click(on_botton3_click)\n",
    "    botton4.on_click(on_botton4_click)\n",
    "    botton5.on_click(on_botton5_click)\n",
    "    botton6.on_click(on_botton6_click)\n",
    "    botton7.on_click(on_botton7_click)\n",
    "    botton8.on_click(on_botton8_click)\n",
    "    clear_button.on_click(on_clear_button_click)\n",
    "\n",
    "    # -- Step 4: Create group buttons with dropdown arrow, bold text, and larger font\n",
    "    # -- Define a consistent layout for button width\n",
    "    group_button_layout = widgets.Layout(width='150px')  # You can adjust the width as needed\n",
    "\n",
    "    # -- Define style for button text\n",
    "    group_button_style = {'font_weight': 'bold', 'font_size': '12px'}\n",
    "    \n",
    "    general_button = widgets.Button(\n",
    "        description=\"General Actions ▼\",\n",
    "        layout=group_button_layout,\n",
    "        style=group_button_style\n",
    "    )\n",
    "    report_rate_botton = widgets.Button(\n",
    "        description=\"Report Rates ▼\",\n",
    "        layout=group_button_layout,\n",
    "        style=group_button_style\n",
    "    )\n",
    "    lga_button = widgets.Button(\n",
    "        description=\"LGA Reports ▼\",\n",
    "        layout=group_button_layout,\n",
    "        style=group_button_style\n",
    "    )\n",
    "    agyw_button = widgets.Button(\n",
    "        description=\"AGYW Reports ▼\",\n",
    "        layout=group_button_layout,\n",
    "        style=group_button_style\n",
    "    )\n",
    "    art_button = widgets.Button(\n",
    "        description=\"ART Reports ▼\",\n",
    "        layout=group_button_layout,\n",
    "        style=group_button_style\n",
    "    )\n",
    "\n",
    "    # -- Define sub-button containers\n",
    "    general_sub_buttons = widgets.HBox([botton0, botton1, clear_button])\n",
    "    report_rate_sub_botton = widgets.HBox([botton2, botton3, clear_button])\n",
    "    agyw_sub_buttons = widgets.HBox([botton4, botton5, botton6, botton7, clear_button])\n",
    "    art_sub_buttons = widgets.HBox([botton8, clear_button])\n",
    "\n",
    "    # -- Track the currently open group\n",
    "    current_open = [None]\n",
    "    sub_button_area = widgets.VBox([])\n",
    "\n",
    "    # -- Step 5: Define group button handlers to toggle sub-buttons\n",
    "    def update_button_descriptions(closed_button, opened_button):\n",
    "        for btn in [general_button, report_rate_botton, agyw_button, art_button]:\n",
    "            if btn != opened_button and btn.description.endswith(\"▲\"):\n",
    "                btn.description = btn.description.replace(\"▲\", \"▼\")\n",
    "        if closed_button and closed_button.description.endswith(\"▲\"):\n",
    "            closed_button.description = closed_button.description.replace(\"▲\", \"▼\")\n",
    "        if opened_button and not opened_button.description.endswith(\"▲\"):\n",
    "            opened_button.description = opened_button.description.replace(\"▼\", \"▲\")\n",
    "\n",
    "    def on_general_button_click(b):\n",
    "        if current_open[0] == general_button:\n",
    "            sub_button_area.children = []\n",
    "            update_button_descriptions(general_button, None)\n",
    "            current_open[0] = None\n",
    "        else:\n",
    "            sub_button_area.children = [general_sub_buttons]\n",
    "            update_button_descriptions(current_open[0], general_button)\n",
    "            current_open[0] = general_button\n",
    "\n",
    "    def on_report_rate_botton_click(b):\n",
    "        if current_open[0] == report_rate_botton:\n",
    "            sub_button_area.children = []\n",
    "            update_button_descriptions(report_rate_botton, None)\n",
    "            current_open[0] = None\n",
    "        else:\n",
    "            sub_button_area.children = [report_rate_sub_botton]\n",
    "            update_button_descriptions(current_open[0], report_rate_botton)\n",
    "            current_open[0] = report_rate_botton\n",
    "\n",
    "    def on_lga_button_click(b):\n",
    "        if current_open[0] == lga_button:\n",
    "            sub_button_area.children = []\n",
    "            update_button_descriptions(lga_button, None)\n",
    "            current_open[0] = None\n",
    "        else:\n",
    "            sub_button_area.children = [lga_sub_buttons]\n",
    "            update_button_descriptions(current_open[0], lga_button)\n",
    "            current_open[0] = lga_button\n",
    "\n",
    "    def on_agyw_button_click(b):\n",
    "        if current_open[0] == agyw_button:\n",
    "            sub_button_area.children = []\n",
    "            update_button_descriptions(agyw_button, None)\n",
    "            current_open[0] = None\n",
    "        else:\n",
    "            sub_button_area.children = [agyw_sub_buttons]\n",
    "            update_button_descriptions(current_open[0], agyw_button)\n",
    "            current_open[0] = agyw_button\n",
    "\n",
    "    def on_art_button_click(b):\n",
    "        if current_open[0] == art_button:\n",
    "            sub_button_area.children = []\n",
    "            update_button_descriptions(art_button, None)\n",
    "            current_open[0] = None\n",
    "        else:\n",
    "            sub_button_area.children = [art_sub_buttons]\n",
    "            update_button_descriptions(current_open[0], art_button)\n",
    "            current_open[0] = art_button\n",
    "\n",
    "    # -- Step 6: Link group buttons to their handlers\n",
    "    general_button.on_click(on_general_button_click)\n",
    "    report_rate_botton.on_click(on_report_rate_botton_click)\n",
    "    agyw_button.on_click(on_agyw_button_click)\n",
    "    art_button.on_click(on_art_button_click)\n",
    "\n",
    "    # -- Step 7: Create a horizontal layout for group buttons\n",
    "    group_buttons = widgets.HBox([\n",
    "        general_button,\n",
    "        report_rate_botton,\n",
    "        agyw_button,\n",
    "        art_button\n",
    "    ], layout=widgets.Layout(\n",
    "        align_items=\"flex-start\",\n",
    "        padding=\"10px\"\n",
    "    ))\n",
    "\n",
    "    # -- Step 8: Create the main layout\n",
    "    layout = widgets.VBox([\n",
    "        group_buttons,\n",
    "        sub_button_area,\n",
    "        output\n",
    "    ], layout=widgets.Layout(\n",
    "        align_items=\"flex-start\",\n",
    "        padding=\"10px\"\n",
    "    ))\n",
    "\n",
    "    # -- Step 9: Display the interface\n",
    "    display(layout)\n",
    "\n",
    "# -- Ensure this is the last cell in your notebook\n",
    "run_jupyter_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d363c-aef3-416f-8d94-9992e657d86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e71f83-e8cb-4ef5-a866-6683ee4e130a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486e543-8e71-40d2-8e98-90edfa0c0c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
